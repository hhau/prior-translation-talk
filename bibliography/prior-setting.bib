@unpublished{abergel_fast_2019,
  title = {Fast and Accurate Evaluation of a Generalized Incomplete Gamma Function},
  author = {Abergel, R{\'e}my and Moisan, Lionel},
  year = {2019},
  month = nov,
  urldate = {2022-03-17},
  abstract = {We present a computational procedure to evaluate the integral {$\int$}xy sp-1 e-{$\mu$}s ds, for 0 {$\leq$} x {$<$} y {$\leq$}+{$\infty$}, {$\mu$} = {$\pm$}1, p {$>$} 0, which generalizes the lower (x=0) and upper (y=+{$\infty$}) incomplete gamma functions. To allow for large values of x, y, and p while avoiding under/overflow issues in the standard double precision floating point arithmetic, we use an explicit normalization that is much more efficient than the classical ratio with the complete gamma function. The generalized incomplete gamma function is estimated with continued fractions, integrations by parts, or, when x {$\approx$} y, with the Romberg numerical integration algorithm. We show that the accuracy reached by our algorithm improves a recent state-of-the-art method by two orders of magnitude, and is essentially optimal considering the limitations imposed by the floating point arithmetic. Moreover, the admissible parameter range of our algorithm (0 {$\leq$} p,x,y {$\leq$} 1015) is much larger than competing algorithms and its robustness is assessed through massive usage in an image processing application.},
  keywords = {broken-with-latex,continued fraction,floating-point arithmetic,incomplete gamma function,incomplete gamma integral,numerical cancellation,Romberg's method}
}

@article{aguilar_intuitive_2022,
  title = {Intuitive Joint Priors for {{Bayesian}} Linear Multilevel Models: {{The R2-D2-M2}} Prior},
  shorttitle = {Intuitive Joint Priors for Bayesian Linear Multilevel Models},
  author = {Aguilar, Javier Enrique and B{\"u}rkner, Paul-Christian},
  year = {2022},
  month = aug,
  journal = {arXiv:2208.07132 [stat]},
  eprint = {2208.07132},
  primaryclass = {stat},
  urldate = {2022-08-17},
  abstract = {The training of high-dimensional regression models on comparably sparse data is an important yet complicated topic, especially when there are many more model parameters than observations in the data. From a Bayesian perspective, inference in such cases can be achieved with the help of shrinkage prior distributions, at least for generalized linear models. However, real-world data usually possess multilevel structures, such as repeated measurements or natural groupings of individuals, which existing shrinkage priors are not built to deal with. We generalize and extend one of these priors, the R2-D2 prior by Zhang et al. (2020), to linear multilevel models leading to what we call the R2-D2-M2 prior. The proposed prior enables both local and global shrinkage of the model parameters. It comes with interpretable hyperparameters, which we show to be intrinsically related to vital properties of the prior, such as rates of concentration around the origin, tail behavior, and amount of shrinkage the prior exerts. We offer guidelines on how to select the prior's hyperparameters by deriving shrinkage factors and measuring the effective number of non-zero model coefficients. Hence, the user can readily evaluate and interpret the amount of shrinkage implied by a specific choice of hyperparameters. Finally, we perform extensive experiments on simulated and real data, showing that our prior is well calibrated, has desirable global and local regularization properties and enables the reliable and interpretable estimation of much more complex Bayesian multilevel models than was previously possible.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation,Statistics - Methodology}
}

@phdthesis{akbarov_probability_2009,
  title = {Probability Elicitation: {{Predictive}} Approach},
  shorttitle = {Probability Elicitation},
  author = {Akbarov, A.},
  year = {2009},
  issn = {1121-7677},
  urldate = {2022-01-16},
  abstract = {Probability elicitation is an important area of research with a wide scope for investigation and experimentation. The existing literature on the subject is vast and spread over many disciplines. This indicates the importance of the subject and the ubiquitous nature of the concept of probability. In this thesis, we focus on a probability elicitation method known as predictive elicitation. Predictive elicitation is a method for estimating hyperparameters of prior distributions by inverting corresponding prior predictive distributions. The uncertainty associated with prior predictive distributions is the uncertainty associated with socalled observable quantities. This uncertainty is generally accepted to be fundamentally more robust for elicitation than the uncertainty about unobservable parameters associated with prior distributions. Although predictive elicitation is the most natural way for eliciting probabilities for Bayesian models, it has two major difficulties for practical implementation. The first of these difficulties is related to inverting integral equations. Here, we deal with this difficulty by restricting the space of possible classes of prior distributions into three families, namely the beta, gamma and normal families as suggested by Percy (2002- 2004). The second difficulty is the problem of constraints on eliciting quantiles of the prior predictive distribution. In this thesis, we propose a method for identifying such constraints for single parameter models. We also propose a computational algorithm that makes predictive elicitation accessible for two-parameter models. We demonstrate that using the proposed elicitation method for two-parameter models it is possible to identify associated constraints. In summary, we extend the current literature related to predictive elicitation by adding to it the following main points: We propose a method for identifying constraints on the elicitation of quantiles for single parameter models. We propose the use of a new hybrid elicitation procedure for two-parameter models. We also investigate a method for identifying constraints on the elicitation process posed by the hybrid elicitation strategy. We provide numerical algorithms, programmed using MathCAD software, that enable full implementation of predictive elicitation for single parameter models. We also provide similar programs for selected two-parameter models that enable implementation of the proposed hybrid elicitation method. These algorithms can be used as bases for developing generic software for implementing predictive elicitation. Further research is needed to address the issue of the practical applicability of predictive elicitation to multi-parameter and multivariate models. The advancements made in this thesis provide foundations and an approach for dealing with the problem of constraints that can be extended to solve similar problems for multi-parameter and multivariate models.},
  collaborator = {Scarf, P. A. and Percy, D. F.},
  copyright = {http://usir.salford.ac.uk/id/eprint/26502/1/11217677.pdf:staffonly},
  langid = {english},
  school = {University of Salford}
}

@article{albert_combining_2012,
  title = {Combining Expert Opinions in Prior Elicitation},
  author = {Albert, Isabelle and Donnet, Sophie and {Guihenneuc-Jouyaux}, Chantal and {Low-Choy}, Samantha and Mengersen, Kerrie and Rousseau, Judith},
  year = {2012},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {7},
  number = {3},
  pages = {503--532},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/12-BA717},
  urldate = {2022-06-13},
  abstract = {We consider the problem of combining opinions from different experts in an explicitly model-based way to construct a valid subjective prior in a Bayesian statistical approach. We propose a generic approach by considering a hierarchical model accounting for various sources of variation as well as accounting for potential dependence between experts. We apply this approach to two problems. The first problem deals with a food risk assessment problem involving modelling dose-response for Listeria monocytogenes contamination of mice. Two hierarchical levels of variation are considered (between and within experts) with a complex mathematical situation due to the use of an indirect probit regression. The second concerns the time taken by PhD students to submit their thesis in a particular school. It illustrates a complex situation where three hierarchical levels of variation are modelled but with a simpler underlying probability distribution (log-Normal).},
  keywords = {Bayesian statistics,hierarchical model,random effects,read,risk assessment}
}

@article{amico_Cure_2018,
  title = {Cure Models in Survival Analysis},
  author = {Amico, Ma{\"i}lis and Van Keilegom, Ingrid},
  year = {2018},
  journal = {Annual Review of Statistics and Its Application},
  volume = {5},
  number = {1},
  pages = {311--342},
  doi = {10.1146/annurev-statistics-031017-100101},
  urldate = {2022-06-08},
  abstract = {When analyzing time-to-event data, it often happens that a certain fraction of the data corresponds to subjects who will never experience the event of interest. These event times are considered as infinite and the subjects are said to be cured. Survival models that take this feature into account are commonly referred to as cure models. This article reviews the literature on cure regression models in which the event time (response) is subject to random right censoring and has a positive probability to be equal to infinity.},
  keywords = {read}
}

@article{anderson_asymptotic_1952,
  title = {Asymptotic Theory of Certain "{{Goodness}} of {{Fit}}" Criteria Based on Stochastic Processes},
  author = {Anderson, T. W. and Darling, D. A.},
  year = {1952},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {2},
  pages = {193--212},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729437},
  urldate = {2022-02-18},
  abstract = {The statistical problem treated is that of testing the hypothesis that \$n\$ independent, identically distributed random variables have a specified continuous distribution function \$F(x)\$. If \$F\_n(x)\$ is the empirical cumulative distribution function and \$\textbackslash psi(t)\$ is some nonnegative weight function \$(0 \textbackslash leqq t \textbackslash leqq 1)\$, we consider \$n\^\{\textbackslash frac\{1\}\{2\}\} \textbackslash sup\_\{-\textbackslash infty\vphantom\}},
  keywords = {read}
}

@article{andrianakis_bayesian_2015,
  title = {Bayesian History Matching of Complex Infectious Disease Models Using Emulation: {{A}} Tutorial and a Case Study on {{HIV}} in {{Uganda}}},
  shorttitle = {Bayesian History Matching of Complex Infectious Disease Models Using Emulation},
  author = {Andrianakis, Ioannis and Vernon, Ian R. and McCreesh, Nicky and McKinley, Trevelyan J. and Oakley, Jeremy E. and Nsubuga, Rebecca N. and Goldstein, Michael and White, Richard G.},
  year = {2015},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {1},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003968},
  urldate = {2022-06-13},
  abstract = {Advances in scientific computing have allowed the development of complex models that are being routinely applied to problems in disease epidemiology, public health and decision making. The utility of these models depends in part on how well they can reproduce empirical data. However, fitting such models to real world data is greatly hindered both by large numbers of input and output parameters, and by long run times, such that many modelling studies lack a formal calibration methodology. We present a novel method that has the potential to improve the calibration of complex infectious disease models (hereafter called simulators). We present this in the form of a tutorial and a case study where we history match a dynamic, event-driven, individual-based stochastic HIV simulator, using extensive demographic, behavioural and epidemiological data available from Uganda. The tutorial describes history matching and emulation. History matching is an iterative procedure that reduces the simulator's input space by identifying and discarding areas that are unlikely to provide a good match to the empirical data. History matching relies on the computational efficiency of a Bayesian representation of the simulator, known as an emulator. Emulators mimic the simulator's behaviour, but are often several orders of magnitude faster to evaluate. In the case study, we use a 22 input simulator, fitting its 18 outputs simultaneously. After 9 iterations of history matching, a non-implausible region of the simulator input space was identified that was times smaller than the original input space. Simulator evaluations made within this region were found to have a 65\% probability of fitting all 18 outputs. History matching and emulation are useful additions to the toolbox of infectious disease modellers. Further research is required to explicitly address the stochastic nature of the simulator as well as to account for correlations between outputs.},
  langid = {english},
  keywords = {Computer modeling,HIV,HIV epidemiology,Human sexual behavior,Infectious disease modeling,Medical risk factors,Normal distribution,read,Uganda}
}

@article{azzalini_multivariate_1996,
  title = {The Multivariate Skew-Normal Distribution},
  author = {Azzalini, A. and Valle, A. Dalla},
  year = {1996},
  month = dec,
  journal = {Biometrika},
  volume = {83},
  number = {4},
  pages = {715--726},
  issn = {0006-3444},
  doi = {10.1093/biomet/83.4.715},
  urldate = {2022-06-06},
  abstract = {The paper extends earlier work on the so-called skew-normal distribution, a family of distributions including the normal, but with an extra parameter to regulate skewness. The present work introduces a multivariate parametric family such that the marginal densities are scalar skew-normal, and studies its properties, with special emphasis on the bivariate case.},
  keywords = {read}
}

@misc{azzalini_sn_2022,
  title = {{{{\textsc{sn}}}}: The Skew-Normal and Related Distributions Such as the Skew-t and the {{SUN}}},
  shorttitle = {Sn},
  author = {Azzalini, Adelchi},
  year = {2022},
  month = mar,
  urldate = {2022-06-06},
  abstract = {Build and manipulate probability distributions of the skew-normal family and some related ones, notably the skew-t and the SUN families. For the skew-normal and the skew-t distributions, statistical methods are provided for data fitting and model diagnostics, in the univariate and the multivariate case.},
  copyright = {GPL-2 | GPL-3},
  keywords = {Distributions,read}
}

@article{bedrick_bayesian_1997,
  title = {Bayesian Binomial Regression: {{Predicting}} Survival at a Trauma Center},
  shorttitle = {Bayesian Binomial Regression},
  author = {Bedrick, Edward J. and Christensen, Ronald and Johnson, Wesley},
  year = {1997},
  journal = {The American Statistician},
  volume = {51},
  number = {3},
  pages = {211--218},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0003-1305},
  doi = {10.2307/2684890},
  urldate = {2022-02-03},
  abstract = {Standard methods for analyzing binomial regression data rely on asymptotic inferences. Bayesian methods can be performed using simple computations, and they apply for any sample size. We provide a relatively complete discussion of Bayesian inferences for binomial regression with emphasis on inferences for the probability of "success." Furthermore, we illustrate diagnostic tools, perform model selection among nonnested models, and examine the sensitivity of the Bayesian methods.},
  jstor = {2684890},
  keywords = {read}
}

@article{beume_complexity_2009,
  title = {On the Complexity of Computing the Hypervolume Indicator},
  author = {Beume, Nicola and Fonseca, Carlos M. and {Lopez-Ibanez}, Manuel and Paquete, Lu{\'I}s and Vahrenhold, Jan},
  year = {2009},
  month = oct,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {13},
  number = {5},
  pages = {1075--1082},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2009.2015575},
  abstract = {The goal of multiobjective optimization is to find a set of best compromise solutions for typically conflicting objectives. Due to the complex nature of most real-life problems, only an approximation to such an optimal set can be obtained within reasonable (computing) time. To compare such approximations, and thereby the performance of multiobjective optimizers providing them, unary quality measures are usually applied. Among these, the hypervolume indicator (or S-metric) is of particular relevance due to its favorable properties. Moreover, this indicator has been successfully integrated into stochastic optimizers, such as evolutionary algorithms, where it serves as a guidance criterion for finding good approximations to the Pareto front. Recent results show that computing the hypervolume indicator can be seen as solving a specialized version of Klee's Measure Problem. In general, Klee's Measure Problem can be solved with \$\textbackslash cal O(n \textbackslash log n + n{\^d}/2\textbackslash log n)\$ comparisons for an input instance of size \$n\$ in \$d\$ dimensions; as of this writing, it is unknown whether a lower bound higher than \$\O mega (n \textbackslash log n)\$ can be proven. In this paper, we derive a lower bound of \$\O mega (n\textbackslash log n)\$ for the complexity of computing the hypervolume indicator in any number of dimensions \$d{$\gg$}1\$ by reducing the so-called uniformgap problem to it. For the 3-D case, we also present a matching upper bound of \$\textbackslash cal O(n\textbackslash log n)\$ comparisons that is obtained by extending an algorithm for finding the maxima of a point set.},
  keywords = {Complexity analysis,computational geometry,Computational geometry,Computer science,Evolutionary computation,Informatics,Laboratories,multiobjective optimization,Pareto optimization,Performance analysis,performance assessment,Size measurement,Stochastic processes,Upper bound}
}

@article{beume_sms-emoa_2007,
  title = {{{SMS-EMOA}}: {{Multiobjective}} Selection Based on Dominated Hypervolume},
  shorttitle = {{{SMS-EMOA}}},
  author = {Beume, Nicola and Naujoks, Boris and Emmerich, Michael},
  year = {2007},
  month = sep,
  journal = {European Journal of Operational Research},
  volume = {181},
  number = {3},
  pages = {1653--1669},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2006.08.008},
  urldate = {2022-08-12},
  abstract = {The hypervolume measure (or S metric) is a frequently applied quality measure for comparing the results of evolutionary multiobjective optimisation algorithms (EMOA). The new idea is to aim explicitly for the maximisation of the dominated hypervolume within the optimisation process. A steady-state EMOA is proposed that features a selection operator based on the hypervolume measure combined with the concept of non-dominated sorting. The algorithm's population evolves to a well-distributed set of solutions, thereby focussing on interesting regions of the Pareto front. The performance of the devised S metric selection EMOA (SMS-EMOA) is compared to state-of-the-art methods on two- and three-objective benchmark suites as well as on aeronautical real-world applications.},
  langid = {english},
  keywords = {Evolutionary computations,Evolutionary multiple objective optimisation,Hypervolume measure,OR in aerodynamic industries,Performance assessment}
}

@article{bhattacharya_Dirichlet_2015,
  title = {Dirichlet\textendash{{Laplace}} Priors for Optimal Shrinkage},
  author = {Bhattacharya, Anirban and Pati, Debdeep and Pillai, Natesh S. and Dunson, David B.},
  year = {2015},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {512},
  pages = {1479--1490},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.960967},
  urldate = {2022-02-15},
  abstract = {Penalized regression methods, such as L1 regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In contrast to the frequentist literature, little is known about the properties of such priors and the convergence and concentration of the corresponding posterior distribution. In this article, we propose a new class of Dirichlet\textendash Laplace priors, which possess optimal posterior concentration and lead to efficient posterior computation. Finite sample performance of Dirichlet\textendash Laplace priors relative to alternatives is assessed in simulated and real data examples.},
  keywords = {Bayesian,Convergence rate,High-dimensional,L1,Lasso,Penalized regression,read,Regularization,Shrinkage prior.}
}

@article{bischl_mlrmbo_2018,
  title = {{{mlrMBO}}: {{A}} Modular Framework for Model-Based Optimization of Expensive Black-Box Functions},
  shorttitle = {Mlrmbo},
  author = {Bischl, Bernd and Richter, Jakob and Bossek, Jakob and Horn, Daniel and Thomas, Janek and Lang, Michel},
  year = {2018},
  month = dec,
  journal = {arXiv:1703.03373 [stat]},
  eprint = {1703.03373},
  primaryclass = {stat},
  urldate = {2021-09-20},
  abstract = {We present mlrMBO, a flexible and comprehensive R toolbox for model-based optimization (MBO), also known as Bayesian optimization, which addresses the problem of expensive black-box optimization by approximating the given objective function through a surrogate regression model. It is designed for both single- and multi-objective optimization with mixed continuous, categorical and conditional parameters. Additional features include multi-point batch proposal, parallelization, visualization, logging and error-handling. mlrMBO is implemented in a modular fashion, such that single components can be easily replaced or adapted by the user for specific use cases, e.g., any regression learner from the mlr toolbox for machine learning can be used, and infill criteria and infill optimizers are easily exchangeable. We empirically demonstrate that mlrMBO provides state-of-the-art performance by comparing it on different benchmark scenarios against a wide range of other optimizers, including DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and Hyperopt.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Machine Learning}
}

@misc{bischl_paramhelpers_2022,
  title = {{{{\textsc{ParamHelpers}}}}: {{Helpers}} for Parameters in Black-Box Optimization, Tuning and Machine Learning},
  shorttitle = {{{ParamHelpers}}},
  author = {Bischl, Bernd and Lang, Michel and Richter, Jakob and Bossek, Jakob and Horn, Daniel and Schork, Karin and Kerschke, Pascal},
  year = {2022},
  month = jul,
  urldate = {2022-08-03},
  abstract = {Functions for parameter descriptions and operations in black-box optimization, tuning and machine learning. Parameters can be described (type, constraints, defaults, etc.), combined to parameter sets and can in general be programmed on. A useful OptPath object (archive) to log function evaluations is also provided.},
  copyright = {BSD\_2\_clause + file LICENSE}
}

@article{bissiri_general_2016,
  title = {A General Framework for Updating Belief Distributions},
  author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
  year = {2016},
  month = nov,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  pages = {1103--1130},
  issn = {13697412},
  doi = {10.1111/rssb.12158},
  urldate = {2020-07-29},
  abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
  langid = {english},
  keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,read,Self-information loss function}
}

@article{blanchard_accurately_2021,
  title = {Accurately Computing the Log-Sum-Exp and Softmax Functions},
  author = {Blanchard, Pierre and Higham, Desmond J and {Nicholas J Higham}},
  year = {2021},
  month = oct,
  journal = {IMA Journal of Numerical Analysis},
  volume = {41},
  number = {4},
  pages = {2311--2330},
  issn = {0272-4979},
  doi = {10.1093/imanum/draa038},
  urldate = {2022-04-25},
  abstract = {Evaluating the log-sum-exp function or the softmax function is a key step in many modern data science algorithms, notably in inference and classification. Because of the exponentials that these functions contain, the evaluation is prone to overflow and underflow, especially in low-precision arithmetic. Software implementations commonly use alternative formulas that avoid overflow and reduce the chance of harmful underflow, employing a shift or another rewriting. Although mathematically equivalent, these variants behave differently in floating-point arithmetic and shifting can introduce subtractive cancellation. We give rounding error analyses of different evaluation algorithms and interpret the error bounds using condition numbers for the functions. We conclude, based on the analysis and numerical experiments, that the shifted formulas are of similar accuracy to the unshifted ones, so can safely be used, but that a division-free variant of softmax can suffer from loss of accuracy.},
  keywords = {read}
}

@mastersthesis{bon_stochastic_2019,
  title = {Stochastic Nets and {{Bayesian}} Regularisation from Constraints},
  author = {Bon, Joshua},
  year = {2019},
  doi = {10.26182/5d088957adaa7},
  abstract = {Regularisation in Bayesian modelling uses classes of priors which encourage shrinkage on posterior quantities. This research proposes a new probabilistic interpretation for regularisation by way of constraints. We augment a given probability distribution with a stochastic constraint that restricts the support of the prior, emitting a "regularised" distribution as a result. This introduces the notion of probabilistic regularisation as an operator that acts on a probability distribution, rather than classes of priors which are considered to have desirable shrinkage properties. Several aspects of the so-called stochastic constraint priors are explored, including full Bayesian computation and theoretical results.},
  keywords = {Bayesian,Constraints,Horseshoe prior,LASSO,MCMC,Prior construction,Regularisation,Sparse regression}
}

@article{box_sampling_1980,
  ids = {box:80},
  title = {Sampling and {{Bayes}}' Inference in Scientific Modelling and Robustness},
  author = {Box, George E. P.},
  year = {1980},
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {143},
  number = {4},
  pages = {383--404},
  publisher = {{Wiley}},
  issn = {2397-2327},
  doi = {10.2307/2982063},
  urldate = {2022-06-13},
  abstract = {Scientific learning is an iterative process employing Criticism and Estimation. Correspondingly the formulated model factors into two complementary parts\textemdash a predictive part allowing model criticism, and a Bayes posterior part allowing estimation. Implications for significance tests, the theory of precise measurement and for ridge estimates are considered. Predictive checking functions for transformation, serial correlation, bad values, and their relation with Bayesian options are considered. Robustness is seen from a Bayesian viewpoint and examples are given. For the bad value problem a comparison with M estimators is made.},
  langid = {english},
  keywords = {bad values,bayes theorem,diagnostic check,inference,iterative learning,model building,outliers,predictive distribution,read,robust estimation,sampling theory,serial correlation,transformations}
}

@misc{bradbury_jax_2018,
  title = {{{{\textsc{JAX}}}}: {{Composable}} Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@article{brown_inference_2010,
  title = {Inference with Normal-Gamma Prior Distributions in Regression Problems},
  author = {Brown, Philip J. and Griffin, Jim E.},
  year = {2010},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {5},
  number = {1},
  pages = {171--188},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/10-BA507},
  urldate = {2022-07-14},
  abstract = {This paper considers the effects of placing an absolutely continuous prior distribution on the regression coefficients of a linear model. We show that the posterior expectation is a matrix-shrunken version of the least squares estimate where the shrinkage matrix depends on the derivatives of the prior predictive density of the least squares estimate. The special case of the normal-gamma prior, which generalizes the Bayesian Lasso (Park and Casella 2008), is studied in depth. We discuss the prior interpretation and the posterior effects of hyperparameter choice and suggest a data-dependent default prior. Simulations and a chemometric example are used to compare the performance of the normal-gamma and the Bayesian Lasso in terms of out-of-sample predictive performance.},
  keywords = {"Spike-and-slab" prior,\$p{$>$}n\$,Bayesian lasso,Markov chain Monte Carlo,multiple regression,normal-gamma prior,Posterior moments,Scale mixture of normals,shrinkage}
}

@article{burkner_brms_2017,
  title = {{{{\textsc{brms}}}}: An {{R}} Package for {{Bayesian}} Multilevel Models Using {{Stan}}},
  shorttitle = {Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {80},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  urldate = {2022-08-26},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
  copyright = {Copyright (c) 2017 Paul-Christian B\"urkner},
  langid = {english},
  keywords = {R}
}

@article{carvalho_normalized_2021,
  title = {On the Normalized Power Prior},
  author = {Carvalho, Luiz Max and Ibrahim, Joseph G.},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {24},
  pages = {5251--5275},
  issn = {1097-0258},
  doi = {10.1002/sim.9124},
  urldate = {2022-06-27},
  abstract = {The power prior is a popular tool for constructing informative prior distributions based on historical data. The method consists of raising the likelihood to a discounting factor in order to control the amount of information borrowed from the historical data. However, one often wishes to assign this discounting factor a prior distribution and estimate it jointly with the parameters, which in turn necessitates the computation of a normalizing constant. In this article, we are concerned with how to approximately sample from joint posterior of the parameters and the discounting factor. We first show a few important properties of the normalizing constant and then use these results to motivate a bisection-type algorithm for computing it on a fixed budget of evaluations. We give a large array of illustrations and discuss cases where the normalizing constant is known in closed-form and where it is not. We show that the proposed method produces approximate posteriors that are very close to the exact distributions and also produces posteriors that cover the data-generating parameters with higher probability in the intractable case. Our results suggest that the proposed method is an accurate and easy to implement technique to include this normalization, being applicable to a large class of models. They also reinforce the notion that proper inclusion of the normalizing constant is crucial to the drawing of correct inferences and appropriate quantification of uncertainty.},
  langid = {english},
  keywords = {doubly intractable,elicitation,historical data,normalization,power prior,read,sensitivity analysis}
}

@book{casella_statistical_1990,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {1990},
  series = {The {{Wadsworth}} \& {{Brooks}}/{{Cole Statistics}}/{{Probability}} Series},
  publisher = {{Brooks/Cole Pub. Co}},
  address = {{Pacific Grove, Calif}},
  isbn = {978-0-534-11958-4},
  lccn = {QA276 .C37 1990},
  keywords = {Mathematical statistics,Probabilities,uncited-but-relevant}
}

@article{chaloner_graphical_1993,
  title = {Graphical Elicitation of a Prior Distribution for a Clinical Trial},
  author = {Chaloner, Kathryn and Church, Timothy and Louis, Thomas A. and Matts, John P.},
  year = {1993},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {42},
  number = {4},
  pages = {341--353},
  issn = {0039-0526},
  doi = {10.2307/2348469},
  urldate = {2022-06-15},
  abstract = {Bayesian methods are potentially useful for the design, monitoring and analysis of clinical trials. These methods, however, require that prior information be quantified and that the methods be robust. This paper describes a method to help quantify beliefs in the form of a prior distribution about regression coefficients in a proportional hazards regression model. The method uses dynamic graphical displays of probability distributions that can be freehand adjusted. The method was developed for, and is applied to, a randomized trial comparing prophylaxes for toxoplasmosis in a population of HIV-positive individuals. Prior distributions from five AIDS experts are elicited. The experts represent a community of consumers of the results of the trial and these prior distributions can be used to try to make the monitoring and analysis of the trial robust.},
  jstor = {2348469},
  keywords = {read}
}

@phdthesis{chandramouli_accommodation_2020,
  title = {Accommodation of Scientific Intuition within Statistical Inference},
  author = {Chandramouli, Suyog Halasinamara},
  year = {2020},
  langid = {english},
  school = {Indiana University}
}

@article{chen_new_1999,
  title = {A {{New Bayesian Model}} for {{Survival Data}} with a {{Surviving Fraction}}},
  author = {Chen, Ming-Hui and Ibrahim, Joseph G. and Sinha, Debajyoti},
  year = {1999},
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {447},
  pages = {909--919},
  issn = {0162-1459},
  doi = {10.2307/2670006},
  urldate = {2022-06-22},
  abstract = {We consider Bayesian methods for right-censored survival data for populations with a surviving (cure) fraction. We propose a model that is quite different from the standard mixture model for cure rates. We provide a natural motivation and interpretation of the model and derive several novel properties of it. First, we show that the model has a proportional hazards structure, with the covariates depending naturally on the cure rate. Second, we derive several properties of the hazard function for the proposed model and establish mathematical relationships with the mixture model for cure rates. Prior elicitation is discussed in detail, and classes of noninformative and informative prior distributions are proposed. Several theoretical properties of the proposed priors and resulting posteriors are derived, and comparisons are made to the standard mixture model. A real dataset from a melanoma clinical trial is discussed in detail.},
  jstor = {2670006},
  keywords = {read}
}

@article{chen_prior_1999,
  title = {Prior Elicitation, Variable Selection and {{Bayesian}} Computation for Logistic Regression Models},
  author = {Chen, Ming-Hui and Ibrahim, Joseph G. and Yiannoutsos, Constantin},
  year = {1999},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {61},
  number = {1},
  pages = {223--242},
  issn = {1369-7412},
  urldate = {2022-06-22},
  abstract = {Bayesian selection of variables is often difficult to carry out because of the challenge in specifying prior distributions for the regression parameters for all possible models, specifying a prior distribution on the model space and computations. We address these three issues for the logistic regression model. For the first, we propose an informative prior distribution for variable selection. Several theoretical and computational properties of the prior are derived and illustrated with several examples. For the second, we propose a method for specifying an informative prior on the model space, and for the third we propose novel methods for computing the marginal distribution of the data. The new computational algorithms only require Gibbs samples from the full model to facilitate the computation of the prior and posterior model probabilities for all possible models. Several properties of the algorithms are also derived. The prior specification for the first challenge focuses on the observables in that the elicitation is based on a prior prediction y{$_0$} for the response vector and a quantity a{$_0$} quantifying the uncertainty in y{$_0$}. Then, y{$_0$} and a{$_0$} are used to specify a prior for the regression coefficients semi-automatically. Examples using real data are given to demonstrate the methodology.},
  jstor = {2680747},
  keywords = {read}
}

@book{cooke_experts_1991,
  title = {Experts in Uncertainty: {{Opinion}} and Subjective Probability in Science},
  shorttitle = {Experts in Uncertainty},
  author = {Cooke, Roger M.},
  year = {1991},
  series = {Environmental Ethics and Science Policy Series},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  isbn = {978-0-19-506465-0},
  langid = {english},
  lccn = {Q175 .C699 1991},
  keywords = {Decision making,Methodology,Philosophy,Probabilities,read,Science,Uncertainty (Information theory)}
}

@article{cornuet_adaptive_2012,
  title = {Adaptive Multiple Importance Sampling},
  author = {CORNUET, JEAN-MARIE and MARIN, JEAN-MICHEL and MIRA, {\relax ANTONIETTA} and ROBERT, CHRISTIAN P.},
  year = {2012},
  journal = {Scandinavian Journal of Statistics},
  volume = {39},
  number = {4},
  pages = {798--812},
  publisher = {{Wiley}},
  issn = {0303-6898},
  urldate = {2022-03-28},
  abstract = {The Adaptive Multiple Importance Sampling algorithm is aimed at an optimal recycling of past simulations in an iterated importance sampling (IS) scheme. The difference with earlier adaptive IS implementations like Population Monte Carlo is that the importance weights of all simulated values, past as well as present, are recomputed at each iteration, following the technique of the deterministic multiple mixture estimator of Owen \& Zhou (J. Amer. Statist. Assoc., 95, 2000, 135). Although the convergence properties of the algorithm cannot be investigated, we demonstrate through a challenging banana shape target distribution and a population genetics example that the improvement brought by this technique is substantial.},
  jstor = {23357226},
  keywords = {read}
}

@article{craig_constructing_1998,
  title = {Constructing {{Partial Prior Specifications}} for {{Models}} of {{Complex Physical Systems}}},
  author = {Craig, Peter S. and Goldstein, Michael and Seheult, Allan H. and Smith, James A.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {47},
  number = {1},
  pages = {37--53},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  urldate = {2022-06-13},
  abstract = {Many large scale problems, praticulary in the physical sciences, are solved using complex, high dimensional models whose outputs, for a given set of inputs, are expensive and time consuming to evaluate. The complexity of such problems forces us to focus attention on those limited aspects of uncertainty which are directly relevant to the tasks for which the model will be used. We discuss methods for constructing the relevant partial prior specifications for these uncertainties, based on the prior covariance structure. Our approach combines two sources of prior knowledge. First, we elicit both qualitative and quantitative prior information based on expert prior judgments, using computer-based elicitation tools for organizing the complex collection of assessments in a systematic way. Secondly, we test and refine these judgments using detailed experiments based on versions of the model which are cheaper to evaluate. Although the approach is quite general, we illustrate it in the context of matching hydrocarbon reservoir history.},
  jstor = {2988426},
  keywords = {read}
}

@inproceedings{craig_pressure_1997,
  title = {Pressure Matching for Hydrocarbon Reservoirs: {{A}} Case Study in the Use of {{Bayes}} Linear Strategies for Large Computer Experiments},
  shorttitle = {Pressure Matching for Hydrocarbon Reservoirs},
  booktitle = {Case {{Studies}} in {{Bayesian Statistics}}},
  author = {Craig, Peter S. and Goldstein, Michael and Seheult, Allan H. and Smith, James A.},
  editor = {Gatsonis, Constantine and Hodges, James S. and Kass, Robert E. and McCulloch, Robert and Rossi, Peter and Singpurwalla, Nozer D.},
  year = {1997},
  series = {Lecture {{Notes}} in {{Statistics}}},
  pages = {37--93},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2290-3_2},
  abstract = {In the oil industry, fluid flow models for reservoirs are usually too complex to be solved analytically and approximate numerical solutions must be obtained using a `reservoir simulator', a complex computer pro\-gram which takes as input descriptions of the reservoir geology. We describe a Bayes linear strategy for history matching; that is, seeking simulator in\-puts for which the outputs match closely to historical production. This approach, which only requires specification of means, variances and covari\-ances, formally combines reservoir engineers' beliefs with data from fast approximations to the simulator. We present an account of our experiences in applying the strategy to match the pressure history of an active reservoir. The methodology is appropriate in a wide variety of applications involving inverse problems in computer experiments.},
  isbn = {978-1-4612-2290-3},
  langid = {english},
  keywords = {Active Variable,History Match,Hydrocarbon Reservoir,Prior Belief,Prior Specification,read}
}

@article{da_silva_prior_2019,
  title = {Prior Specification via Prior Predictive Matching: {{Poisson}} Matrix Factorization and Beyond},
  shorttitle = {Prior Specification via Prior Predictive Matching},
  author = {{da Silva}, Eliezer de Souza and Ku{\'s}mierczyk, Tomasz and Hartmann, Marcelo and Klami, Arto},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.12263 [cs, stat]},
  eprint = {1910.12263},
  primaryclass = {cs, stat},
  urldate = {2022-02-02},
  abstract = {Hyperparameter optimization for machine learning models is typically carried out by some sort of cross-validation procedure or global optimization, both of which require running the learning algorithm numerous times. We show that for Bayesian hierarchical models there is an appealing alternative that allows selecting good hyperparameters without learning the model parameters during the process at all, facilitated by the prior predictive distribution that marginalizes out the model parameters. We propose an approach that matches suitable statistics of the prior predictive distribution with ones provided by an expert and apply the general concept for matrix factorization models. For some Poisson matrix factorization models we can analytically obtain exact hyperparameters, including the number of factors, and for more complex models we propose a model-independent optimization procedure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read,Statistics - Machine Learning}
}

@inproceedings{daulton_parallel_2021,
  title = {Parallel {{Bayesian Optimization}} of {{Multiple Noisy Objectives}} with {{Expected Hypervolume Improvement}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  year = {2021},
  volume = {34},
  pages = {2187--2200},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-09-23},
  abstract = {Optimizing multiple competing black-box objectives is a challenging problem in many fields, including science, engineering, and machine learning. Multi-objective Bayesian optimization (MOBO) is a sample-efficient approach for identifying the optimal trade-offs between the objectives. However, many existing methods perform poorly when the observations are corrupted by noise. We propose a novel acquisition function, NEHVI, that overcomes this important practical limitation by applying a Bayesian treatment to the popular expected hypervolume improvement (EHVI) criterion and integrating over this uncertainty in the Pareto frontier. We argue that, even in the noiseless setting, generating multiple candidates in parallel is an incarnation of EHVI with uncertainty in the Pareto frontier and therefore can be addressed using the same underlying technique. Through this lens, we derive a natural parallel variant, qNEHVI, that reduces computational complexity of parallel EHVI from exponential to polynomial with respect to the batch size. qNEHVI is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments, and we show that it can be optimized effectively with gradient-based methods via sample average approximation. Empirically, we demonstrate not only that qNEHVI is substantially more robust to observation noise than existing MOBO approaches, but also that it achieves state-of-the-art optimization performance and competitive wall-times in large-batch environments.}
}

@article{deb_fast_2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA-II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {6},
  number = {2},
  pages = {182--197},
  issn = {1941-0026},
  doi = {10.1109/4235.996017},
  abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed.},
  keywords = {Associate members,Computational complexity,Computational modeling,Constraint optimization,Decision making,Diversity reception,Evolutionary computation,Genetic algorithms,Sorting,Testing}
}

@book{deb_multi-objective_2001,
  title = {Multi-Objective Optimization Using Evolutionary Algorithms},
  author = {Deb, Kalyanmoy},
  year = {2001},
  edition = {1st ed},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester; New York}},
  isbn = {978-0-471-87339-6},
  lccn = {QA402.5 .D32 2001},
  keywords = {Evolutionary programming (Computer science),Mathematical optimization,Multiple criteria decision making}
}

@article{dickey_beliefs_1980,
  title = {Beliefs about Beliefs, a Theory for Stochastic Assessments of Subjective Probabilities},
  author = {Dickey, J. M.},
  year = {1980},
  month = feb,
  journal = {Trabajos de Estadistica Y de Investigacion Operativa},
  volume = {31},
  number = {1},
  pages = {471--487},
  issn = {0041-0241},
  doi = {10.1007/BF02888364},
  urldate = {2022-07-27},
  abstract = {Parameterized families of subjective probability distributions can be used to great advantage to model beliefs of experts, especially when such models include dependence on concomitant variables. In one such model, probabilities of simple events can be expressed in loglinear form. In another, a generalization of the multivariate t distribution has concomitant variables entering linearly through the location vector. Interactive interview methods for assessing this second model and matrix extensions thereof were given in recent joint work of the author with A.P. Dawid, J.B. Kadane and others. In any such verbal assessment method, elicited quantiles must be fitted by subjective probability models. The fitting requires the use of a further probability model for errors of elicitation. This paper gives new theory relating the form of the distribution of elicited probabilities and elicited quantiles to the form of the subjective probability distribution. The first and second order moment structures are developed to permit generalized least squares fits.},
  langid = {english}
}

@article{domke_importance_2018,
  title = {Importance Weighting and Variational Inference},
  author = {Domke, Justin and Sheldon, Daniel},
  year = {2018},
  month = oct,
  journal = {arXiv:1808.09034 [cs, stat]},
  eprint = {1808.09034},
  primaryclass = {cs, stat},
  urldate = {2021-07-21},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{draper_coherence_2006,
  title = {Coherence and Calibration: {{Comments}} on Subjectivity and "objectivity'' in {{Bayesian}} Analysis (Comment on Articles by {{Berger}} and by {{Goldstein}})},
  shorttitle = {Coherence and Calibration},
  author = {Draper, David},
  year = {2006},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {3},
  pages = {423--428},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA116B},
  urldate = {2022-02-16},
  abstract = {In this contribution to the discussion of "The case for objective Bayesian analysis" by James Berger and "Subjective Bayesian analysis: principles and practice" by Michael Goldstein, I argue that (a) all Bayesian work is inherently subjective and needs to be guided simultaneously by considerations of both coherence and calibration, and (b) "objective" (diffuse) prior distributions are sometimes, but not always, useful in attaining good calibrative performance\textemdash it depends (as usual) on your judgment about how knowns (e.g., past observables) and unknowns (e.g., future observables) are related.},
  keywords = {broken-with-latex,Meta-analysis,out-of-sample predictive calibration}
}

@article{drovandi_comparison_2021,
  title = {A {{Comparison}} of {{Likelihood-Free Methods With}} and {{Without Summary Statistics}}},
  author = {Drovandi, Christopher and Frazier, David T.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.02407 [stat]},
  eprint = {2103.02407},
  primaryclass = {stat},
  urldate = {2022-02-08},
  abstract = {Likelihood-free methods are useful for parameter estimation of complex models with intractable likelihood functions for which it is easy to simulate data. Such models are prevalent in many disciplines including genetics, biology, ecology and cosmology. Likelihood-free methods avoid explicit likelihood evaluation by finding parameter values of the model that generate data close to the observed data. The general consensus has been that it is most efficient to compare datasets on the basis of a low dimensional informative summary statistic, incurring information loss in favour of reduced dimensionality. More recently, researchers have explored various approaches for efficiently comparing empirical distributions in the likelihood-free context in an effort to avoid data summarisation. This article provides a review of these full data distance based approaches, and conducts the first comprehensive comparison of such methods, both qualitatively and empirically. We also conduct a substantive empirical comparison with summary statistic based likelihood-free methods. The discussion and results offer guidance to practitioners considering a likelihood-free approach. Whilst we find the best approach to be problem dependent, we also find that the full data distance based approaches are promising and warrant further development. We discuss some opportunities for future research in this space.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation,Statistics - Methodology,uncited-but-relevant}
}

@article{drovandi_population_2022,
  title = {Population {{Calibration}} Using {{Likelihood-Free Bayesian Inference}}},
  author = {Drovandi, Christopher and Lawson, Brodie and Jenner, Adrianne L. and Browning, Alexander P.},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.01962 [stat]},
  eprint = {2202.01962},
  primaryclass = {stat},
  urldate = {2022-02-07},
  abstract = {In this paper we develop a likelihood-free approach for population calibration, which involves finding distributions of model parameters when fed through the model produces a set of outputs that matches available population data. Unlike most other approaches to population calibration, our method produces uncertainty quantification on the estimated distribution. Furthermore, the method can be applied to any population calibration problem, regardless of whether the model of interest is deterministic or stochastic, or whether the population data is observed with or without measurement error. We demonstrate the method on several examples, including one with real data. We also discuss the computational limitations of the approach. Immediate applications for the methodology developed here exist in many areas of medical research including cancer, COVID-19, drug development and cardiology.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Computation,Statistics - Methodology}
}

@article{drovandi_semiautomatic_2021,
  title = {A {{Semiautomatic Method}} for {{History Matching Using Sequential Monte Carlo}}},
  author = {Drovandi, Christopher and Nott, David J. and Pagendam, Daniel E.},
  year = {2021},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {9},
  number = {3},
  pages = {1034--1063},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/19M1286694},
  urldate = {2022-09-23},
  abstract = {The aim of the history matching method is to locate nonimplausible regions of the parameter space of complex deterministic or stochastic models by matching model outputs with data. It does this via a series of waves where at each wave an emulator is fitted to a small number of training samples. An implausibility measure is defined which takes into account the closeness of simulated and observed outputs as well as emulator uncertainty. As the waves progress, the emulator becomes more accurate so that training samples are more concentrated on promising regions of the space and poorer parts of the space are rejected with more confidence. While history matching has proved to be useful, existing implementations are not fully automated, and some ad hoc choices are made during the process, which involves user intervention and is time consuming. This occurs especially when the nonimplausible region becomes small and it is difficult to sample this space uniformly to generate new training points. In this article we develop a sequential Monte Carlo (SMC) algorithm for implementing history matching that is semiautomated. Our novel SMC approach reveals that the history matching method yields a nonimplausible region that can be multimodal, highly irregular, and very difficult to sample uniformly. Our SMC approach offers a much more reliable sampling of the nonimplausible space, which requires additional computation compared to other approaches used in the literature.},
  keywords = {62-08,62F15,62F35,62K20,62L12,62M05,emulator,Gaussian process,history matching,hydrology,Markov chain Monte Carlo,Markov processes}
}

@article{duan_bayesian_2020,
  title = {Bayesian Constraint Relaxation},
  author = {Duan, Leo L and Young, Alexander L and Nishimura, Akihiko and Dunson, David B},
  year = {2020},
  month = mar,
  journal = {Biometrika},
  volume = {107},
  number = {1},
  pages = {191--204},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz069},
  urldate = {2022-06-22},
  abstract = {Prior information often takes the form of parameter constraints. Bayesian methods include such information through prior distributions having constrained support. By using posterior sampling algorithms, one can quantify uncertainty without relying on asymptotic approximations. However, sharply constrained priors are not necessary in some settings and tend to limit modelling scope to a narrow set of distributions that are tractable computationally. We propose to replace the sharp indicator function of the constraint with an exponential kernel, thereby creating a close-to-constrained neighbourhood within the Euclidean space in which the constrained subspace is embedded. This kernel decays with distance from the constrained space at a rate depending on a relaxation hyperparameter. By avoiding the sharp constraint, we enable use of off-the-shelf posterior sampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic computation in a broad range of models. We study the constrained and relaxed distributions under multiple settings and theoretically quantify their differences. Application of the method is illustrated through several novel modelling examples.},
  keywords = {uncited-but-relevant}
}

@misc{dunson_approximate_nodate,
  title = {Approximate {{Bayesian Inference}} for {{Quantiles}}},
  author = {Dunson, David B. and Taylor, Jack A. and Branch, Biostatistics and Branch, Epidemiology},
  abstract = {Suppose data consist of a random sample from a distribution function FY, which is unknown, and that interest focuses on inferences on \texttheta, a vector of quantiles of FY. When the likelihood function is not fully specified, a posterior density cannot be calculated and Bayesian infer-ence is difficult. This article considers an approach which relies on a substitution likelihood characterized by a vector of quantiles. Properties of the substitution likelihood are investi-gated, strategies for prior elicitation are presented, and a general framework is proposed for quantile regression modeling. Posterior computation proceeds via a Metropolis algorithm that utilizes a normal approximation to the posterior. Results from a simulation study are presented, and the methods are illustrated through application to data from a genotoxicity experiment.},
  keywords = {broken-with-latex}
}

@article{dustin_predictive_2022,
  title = {Predictive {{Criteria}} for {{Prior Selection Using Shrinkage}} in {{Linear Models}}},
  author = {Dustin, Dean and Clarke, Bertrand and Clarke, Jennifer},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.02244 [stat]},
  eprint = {2201.02244},
  primaryclass = {stat},
  urldate = {2022-01-13},
  abstract = {Choosing a shrinkage method can be done by selecting a penalty from a list of pre-specified penalties or by constructing a penalty based on the data. If a list of penalties for a class of linear models is given, we provide comparisons based on sample size and number of non-zero parameters under a predictive stability criterion based on data perturbation. These comparisons provide recommendations for penalty selection in a variety of settings. If the preference is to construct a penalty customized for a given problem, then we propose a technique based on genetic algorithms, again using a predictive criterion. We find that, in general, a custom penalty never performs worse than any commonly used penalties but that there are cases the custom penalty reduces to a recognizable penalty. Since penalty selection is mathematically equivalent to prior selection, our method also constructs priors. The techniques and recommendations we offer are intended for finite sample cases. In this context, we argue that predictive stability under perturbation is one of the few relevant properties that can be invoked when the true model is not known. Nevertheless, we study variable inclusion in simulations and, as part of our shrinkage selection strategy, we include oracle property considerations. In particular, we see that the oracle property typically holds for penalties that satisfy basic regularity conditions and therefore is not restrictive enough to play a direct role in penalty selection. In addition, our real data example also includes considerations merging from model mis-specification.},
  archiveprefix = {arxiv},
  keywords = {broken-with-latex,Statistics - Methodology}
}

@inproceedings{eriksson_scalable_2019,
  title = {Scalable Global Optimization via Local {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-03-15}
}

@inproceedings{eriksson_scalable_2021,
  title = {Scalable Constrained {{Bayesian}} Optimization},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Eriksson, David and Poloczek, Matthias},
  year = {2021},
  month = mar,
  pages = {730--738},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-04-25},
  abstract = {The global optimization of a high-dimensional black-box function under black-box constraints is a pervasive task in machine learning, control, and engineering. These problems are challenging since the feasible set is typically non-convex and hard to find, in addition to the curses of dimensionality and the heterogeneity of the underlying functions. In particular, these characteristics dramatically impact the performance of Bayesian optimization methods, that otherwise have become the defacto standard for sample-efficient optimization in unconstrained settings, leaving practitioners with evolutionary strategies or heuristics. We propose the scalable constrained Bayesian optimization (SCBO) algorithm that overcomes the above challenges and pushes the applicability of Bayesian optimization far beyond the state-of-the-art. A comprehensive experimental evaluation demonstrates that SCBO achieves excellent results on a variety of benchmarks. To this end, we propose two new control problems that we expect to be of independent value for the scientific community.},
  langid = {english}
}

@article{falconer_methods_2021,
  title = {Methods for Eliciting Informative Prior Distributions},
  author = {Falconer, Julia R. and Frank, Eibe and Polaschek, Devon L. L. and Joshi, Chaitanya},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.07090 [stat]},
  eprint = {2112.07090},
  primaryclass = {stat},
  urldate = {2022-01-24},
  abstract = {Eliciting informative prior distributions for Bayesian inference can often be complex and challenging. While popular methods rely on asking experts probability based questions to quantify uncertainty, these methods are not without their drawbacks and many alternative elicitation methods exist. This paper explores methods for eliciting informative priors categorized by type and briefly discusses their strengths and limitations. Two representative applications are used throughout to explore the suitability, or lack thereof, of the existing methods for eliciting informative priors for these problems. The primary aim of this work is to highlight some of the gaps in the present state of art and identify directions for future research.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Applications,Statistics - Methodology}
}

@article{farewelluse1982,
  title = {The Use of Mixture Models for the Analysis of Survival Data with Long-Term Survivors},
  author = {Farewell, V. T.},
  year = {1982},
  journal = {Biometrics. Journal of the International Biometric Society},
  volume = {38},
  number = {4},
  pages = {1041--1046},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529885},
  urldate = {2022-06-08},
  abstract = {A toxicological experiment analyzed by Pierce, Stewart and Kopecky (1979, Biometrics 35, 758-793) using Cox's proportional-hazards model is reanalyzed using mixture models which postulate a fraction of long-term survivors. The conclusions are seen to be potentially useful due to their simplicity and interpretability. Caution on the general use of mixture models is nevertheless advised.},
  jstor = {2529885}
}

@inproceedings{figurnov_implicit_2018,
  title = {Implicit Reparameterization Gradients},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Figurnov, Mikhail and Mohamed, Shakir and Mnih, Andriy},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-09-23},
  abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.}
}

@inproceedings{fleischer_measure_2003,
  title = {The {{Measure}} of {{Pareto Optima Applications}} to {{Multi-objective Metaheuristics}}},
  booktitle = {Evolutionary {{Multi-Criterion Optimization}}},
  author = {Fleischer, M.},
  editor = {Fonseca, Carlos M. and Fleming, Peter J. and Zitzler, Eckart and Thiele, Lothar and Deb, Kalyanmoy},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {519--533},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36970-8_37},
  abstract = {This article describes a set function that maps a set of Pareto optimal points to a scalar. A theorem is presented that shows that the maximization of this scalar value constitutes the necessary and sufficient condition for the function's arguments to be maximally diverse Pareto optimal solutions of a discrete, multi-objective, optimization problem. This scalar quantity, a hypervolume based on a Lebesgue measure, is therefore the best metric to assess the quality of multiobjective optimization algorithms. Moreover, it can be used as the objective function in simulated annealing (SA) to induce convergence in probability to the Pareto optima. An efficient, polynomial-time algorithm for calculating this scalar and an analysis of its complexity is also presented.},
  isbn = {978-3-540-36970-7},
  langid = {english},
  keywords = {Multiobjective Optimization,Objective Function,Pareto Optimal Solution,Pareto Optimum,Simulated Annealing}
}

@article{fong_martingale_2021-1,
  title = {Martingale Posterior Distributions},
  author = {Fong, Edwin and Holmes, Chris and Walker, Stephen G.},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.15671 [math, stat]},
  eprint = {2103.15671},
  primaryclass = {math, stat},
  urldate = {2022-01-31},
  abstract = {The prior distribution on parameters of a sampling distribution is the usual starting point for Bayesian uncertainty quantification. In this paper, we present a different perspective which focuses on missing observations as the source of statistical uncertainty, with the parameter of interest being known precisely given the entire population. We argue that the foundation of Bayesian inference is to assign a distribution on missing observations conditional on what has been observed. In the conditionally i.i.d. setting with an observed sample of size \$n\$, the Bayesian would thus assign a predictive distribution on the missing \$Y\_\{n+1:\textbackslash infty\}\$ conditional on \$Y\_\{1:n\}\$, which then induces a distribution on the parameter. Demonstrating an application of martingales, Doob shows that choosing the Bayesian predictive distribution returns the conventional posterior as the distribution of the parameter. Taking this as our cue, we relax the predictive machine, avoiding the need for the predictive to be derived solely from the usual prior to posterior to predictive density formula. We introduce the \textbackslash textit\{martingale posterior distribution\}, which returns Bayesian uncertainty directly on any statistic of interest without the need for the likelihood and prior, and this distribution can be sampled through a computational scheme we name \textbackslash textit\{predictive resampling\}. To that end, we introduce new predictive methodologies for multivariate density estimation, regression and classification that build upon recent work on bivariate copulas.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@phdthesis{fong_predictive_2021,
  title = {The Predictive View of {{Bayesian}} Inference},
  author = {Fong, Edwin},
  year = {2021},
  langid = {english},
  school = {University of Oxford}
}

@book{fonseca_evolutionary_2003,
  title = {Evolutionary Multi Criterion Optimization: {{Second International Conference}}, {{EMO}} 2003, {{Faro}}, {{Portugal}}, {{April}} 8 - 11, 2003; Proceedings},
  shorttitle = {Evolutionary Multi Criterion Optimization},
  editor = {{da Fonseca}, Carlos M.},
  year = {2003},
  series = {Lecture Notes in Computer Science},
  number = {2632},
  publisher = {{Springer}},
  address = {{Berlin Heidelberg}},
  isbn = {978-3-540-01869-8},
  langid = {english}
}

@article{frazier_tutorial_2018,
  title = {A Tutorial on {{Bayesian}} Optimization},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.02811 [cs, math, stat]},
  eprint = {1807.02811},
  primaryclass = {cs, math, stat},
  urldate = {2021-09-15},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,read,Statistics - Machine Learning}
}

@article{gabry_visualization_2019,
  ids = {gabry_visualization_2019-1},
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  number = {2},
  pages = {389--402},
  doi = {10.1111/rssa.12378},
  keywords = {Bayesian data analysis,read,Statistical graphics,Statistical workflow}
}

@book{garnett_bayesian_2022,
  title = {Bayesian {{Optimization}}},
  author = {Garnett, Roman},
  year = {2022},
  publisher = {{Cambridge University Press}}
}

@article{garthwaite_statistical_2005,
  ids = {garthwaite_statistical_2005-1},
  title = {Statistical Methods for Eliciting Probability Distributions},
  author = {Garthwaite, Paul H. and Kadane, Joseph B. and O'Hagan, Anthony},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {680--700},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  urldate = {2022-01-11},
  abstract = {Elicitation is a key task for subjectivist Bayesians. Although skeptics hold that elicitation cannot (or perhaps should not) be done, in practice it brings statisticians closer to their clients and subject-matter expert colleagues. This article reviews the state of the art, reflecting the experience of statisticians informed by the fruits of a long line of psychological research into how people represent uncertain information cognitively and how they respond to questions about that information. In a discussion of the elicitation process, the first issue to address is what it means for an elicitation to be successful; that is, what criteria should be used. Our answer is that a successful elicitation faithfully represents the opinion of the person being elicited. It is not necessarily "true" in some objectivistic sense, and cannot be judged in that way. We see that elicitation as simply part of the process of statistical modeling. Indeed, in a hierarchical model at which point the likelihood ends and the prior begins is ambiguous. Thus the same kinds of judgment that inform statistical modeling in general also inform elicitation of prior distributions. The psychological literature suggests that people are prone to certain heuristics and biases in how they respond to situations involving uncertainty. As a result, some of the ways of asking questions about uncertain quantities are preferable to others, and appear to be more reliable. However, data are lacking on exactly how well the various methods work, because it is unclear, other than by asking using an elicitation method, just what the person believes. Consequently, one is reduced to indirect means of assessing elicitation methods. The tool chest of methods is growing. Historically, the first methods involved choosing hyperparameters using conjugate prior families, at a time when these were the only families for which posterior distributions could be computed. Modern computational methods, such as Markov chain Monte Carlo, have freed elicitation from this constraint. As a result, now both parametric and nonparametric methods are available for low-dimensional problems. High-dimensional problems are probably best thought of as lacking another hierarchical level, which has the effect of reducing the as-yet-unelicited parameter space. Special considerations apply to the elicitation of group opinions. Informal methods, such as Delphi, encourage the participants to discuss the issue in the hope of reaching consensus. Formal methods, such as weighted averages or logarithmic opinion pools, each have mathematical characteristics that are uncomfortable. Finally, there is the question of what a group opinion even means, because it is not necessarily the opinion of any participant.},
  jstor = {27590587},
  keywords = {read}
}

@article{gaskins_sparsity_2014,
  title = {Sparsity {{Inducing Prior Distributions}} for {{Correlation Matrices}} of {{Longitudinal Data}}},
  author = {Gaskins, J. T. and Daniels, M. J. and Marcus, B. H.},
  year = {2014},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  number = {4},
  pages = {966--984},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  urldate = {2022-06-22},
  abstract = {For longitudinal data, the modeling of a correlation matrix R can be a difficult statistical task due to both the positive definite and the unit diagonal constraints. Because the number of parameters increases quadratically in the dimension, it is often useful to consider a sparse parameterization. We introduce a pair of prior distributions on the set of correlation matrices for longitudinal data through the partial autocorrelations (PACs), which vary independently over (-1,1). The first prior shrinks each of the PACs toward zero with increasingly aggressive shrinkage in lag. The second prior (a selection prior) is a mixture of a zero point mass and a continuous component for each PAC, allowing for a sparse representation. The structure implied under our priors is readily interpretable for time-ordered responses because each zero PAC implies a conditional independence relationship in the distribution of the data. Selection priors on the PACs provide a computationally attractive alternative to selection on the elements of R or R{$^{-1}$} for ordered data. These priors allow for data-dependent shrinkage/selection under an intuitive parameterization in an unconstrained setting. The proposed priors are compared to standard methods through a simulation study and illustrated using a multivariate probit data example. Supplemental materials for this article (appendix, data, and R code) are available online.},
  jstor = {43304793},
  keywords = {broken-with-latex}
}

@book{gaspar-cunha_evolutionary_2015,
  title = {Evolutionary {{Multi-Criterion Optimization}}: 8th {{International Conference}}, {{EMO}} 2015, {{Guimar\~aes}}, {{Portugal}}, {{March}} 29 \textendash{{April}} 1, 2015. {{Proceedings}}, {{Part I}}},
  shorttitle = {Evolutionary {{Multi-Criterion Optimization}}},
  editor = {{Gaspar-Cunha}, Ant{\'o}nio and Henggeler Antunes, Carlos and Coello, Carlos Coello},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9018},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-15934-8},
  urldate = {2022-04-01},
  isbn = {978-3-319-15933-1 978-3-319-15934-8},
  langid = {english}
}

@article{gelman_bayesian_2020,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  primaryclass = {stat},
  urldate = {2021-01-12},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Methodology}
}

@article{gelman_prior_2017,
  ids = {gelman_prior_2017-1},
  title = {The Prior Can Often Only Be Understood in the Context of the Likelihood},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  year = {2017},
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {19},
  number = {10},
  pages = {555},
  publisher = {{MDPI AG}},
  doi = {10.3390/e19100555},
  keywords = {Bayesian inference,default priors,prior distribution,read}
}

@article{gelman_r-squared_2019,
  title = {R-Squared for {{Bayesian Regression Models}}},
  author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
  year = {2019},
  month = jul,
  journal = {The American Statistician},
  volume = {73},
  number = {3},
  pages = {307--309},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1549100},
  urldate = {2022-01-18},
  abstract = {The usual definition of R2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors.},
  langid = {english}
}

@article{gneiting_strictly_2007,
  title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  urldate = {2021-07-26},
  langid = {english},
  keywords = {read}
}

@article{good_bayesian_1967,
  title = {A {{Bayesian}} Significance Test for Multinomial Distributions},
  author = {Good, I. J.},
  year = {1967},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {29},
  number = {3},
  pages = {399--418},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1967.tb00705.x},
  urldate = {2022-06-23},
  abstract = {A Bayesian significance test for multinomial distributions is discussed, together with the results of 18 numerical experiments in which the test is compared with non-Bayesian methods. Several different non-Bayesian criteria are considered because the circumstances under which their tail-area probabilities can be conveniently approximated differ from one to the other. A provisional empirical formula, connecting the Bayes factors with the tailarea probabilities, is found to be correct within a factor of 6 in the 18 experiments. As a by-product, a new non-Bayesian statistic is suggested by the theory, and its asymptotic distribution obtained. It seems to be useful sometimes when chi-squared is not, although chi-squared also has a Bayesian justification for large samples.},
  langid = {english},
  keywords = {read}
}

@book{good_good_1983,
  title = {Good Thinking: {{The}} Foundations of Probability and Its Applications},
  shorttitle = {Good Thinking},
  author = {Good, Irving John},
  year = {1983},
  publisher = {{University of Minnesota Press}},
  address = {{Minneapolis}},
  isbn = {978-0-8166-1141-6 978-0-8166-1142-3},
  langid = {english},
  lccn = {QA273.4 .G66 1983},
  keywords = {Probabilities}
}

@misc{goodrich_bayesian_2020,
  title = {Bayesian {{Inference}} without {{Probability Density Functions}}},
  author = {Goodrich, Ben},
  year = {2020},
  month = aug,
  urldate = {2022-01-14},
  abstract = {There are too many probability distributions, too many parameterizations of the same probability distribution, and too many differences in notation even for the same parameterization. Worse, the common probability distributions were derived in the pre-computer era to have explicit expressions for their central moments, without considering whether they would also be suitable for use in Stan where everything is estimated using simulations from a posterior distribution rather than analytically. Worse still, the Markov Chain Monte Carlo algorithm in Stan relies on the Probability Density Function (PDF) for a probability distribution, which is an unintuitive and poorly understood concept. All of these factors combine to make it difficult to teach Bayesian inference and difficult for Stan users to specify priors over unknown parameters. Although the algorithm in Stan relies on logarithms of PDFs, the Stan language does not require PDFs at all, despite what its documentation might seem to suggest. This talk will demonstrate how prior beliefs about the unknown parameters can instead be conveyed through transformations via the quantile or inverse Cumulative Distribution Function of a probability distribution. Not only does this eliminate the need for PDFs, it allows Stan users to express their prior beliefs through functions of quantiles (such as the median) that always exist, rather than functions of central moments (such as the mean) that may not exist or be finite and are otherwise difficult to think about because they are defined by integrals. Finally, we can use this opportunity to move toward Keelin and Powley's (2011) "quantile parameterized distributions", which are essentially probability distributions that have explicit quantile functions with several hyperparameters that are, in turn, linear functions of user-specified quantiles. Doing so allows teachers and researchers to focus on a repeatable process for specifying prior beliefs without having to consider so many probability distributions.},
  copyright = {GPL-3.0},
  keywords = {read}
}

@article{goudie_joining_2019,
  ids = {goudie:etal:18},
  title = {Joining and Splitting Models with {{Markov}} Melding},
  author = {Goudie, Robert J. B. and Presanis, Anne M. and Lunn, David and De Angelis, Daniela and Wernisch, Lorenz},
  year = {2019},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {1},
  pages = {81--109},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975},
  doi = {10.1214/18-BA1104},
  urldate = {2020-07-29},
  abstract = {Analysing multiple evidence sources is often feasible only via a modular approach, with separate submodels specified for smaller components of the available evidence. Here we introduce a generic framework that enables fully Bayesian analysis in this setting. We propose a generic method for forming a suitable joint model when joining submodels, and a convenient computational algorithm for fitting this joint model in stages, rather than as a single, monolithic model. The approach also enables splitting of large joint models into smaller submodels, allowing inference for the original joint model to be conducted via our multi-stage algorithm. We motivate and demonstrate our approach through two examples: joining components of an evidence synthesis of A/H1N1 influenza, and splitting a large ecology model.},
  langid = {english}
}

@article{gribok_backward_2004,
  title = {Backward Specification of Prior in {{Bayesian}} Inference as an Inverse Problem},
  author = {Gribok, Andrei V. and Urmanov, Aleksey M. and Wesley Hines, J. and Uhrig, Robert E.},
  year = {2004},
  month = jun,
  journal = {Inverse Problems in Science and Engineering},
  volume = {12},
  number = {3},
  pages = {263--278},
  issn = {1741-5977, 1741-5985},
  doi = {10.1080/10682760310001598689},
  urldate = {2022-01-11},
  langid = {english},
  keywords = {read}
}

@book{gustafson_bayesian_2015,
  title = {Bayesian {{Inference}} for {{Partially Identified Models}}: {{Exploring}} the {{Limits}} of {{Limited Data}}},
  author = {Gustafson, Paul},
  year = {2015},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  isbn = {978-1-4398-6940-6},
  langid = {english},
  keywords = {read}
}

@article{gutenkunst_universally_2007,
  title = {Universally Sloppy Parameter Sensitivities in Systems Biology Models},
  author = {Gutenkunst, Ryan N. and Waterfall, Joshua J. and Casey, Fergal P. and Brown, Kevin S. and Myers, Christopher R. and Sethna, James P.},
  year = {2007},
  month = oct,
  journal = {PLoS computational biology},
  volume = {3},
  number = {10},
  pages = {1871--1878},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.0030189},
  abstract = {Quantitative computational models play an increasingly important role in modern biology. Such models typically involve many free parameters, and assigning their values is often a substantial obstacle to model development. Directly measuring in vivo biochemical parameters is difficult, and collectively fitting them to other experimental data often yields large parameter uncertainties. Nevertheless, in earlier work we showed in a growth-factor-signaling model that collective fitting could yield well-constrained predictions, even when it left individual parameters very poorly constrained. We also showed that the model had a "sloppy" spectrum of parameter sensitivities, with eigenvalues roughly evenly distributed over many decades. Here we use a collection of models from the literature to test whether such sloppy spectra are common in systems biology. Strikingly, we find that every model we examine has a sloppy spectrum of sensitivities. We also test several consequences of this sloppiness for building predictive models. In particular, sloppiness suggests that collective fits to even large amounts of ideal time-series data will often leave many parameters poorly constrained. Tests over our model collection are consistent with this suggestion. This difficulty with collective fits may seem to argue for direct parameter measurements, but sloppiness also implies that such measurements must be formidably precise and complete to usefully constrain many model predictions. We confirm this implication in our growth-factor-signaling model. Our results suggest that sloppy sensitivity spectra are universal in systems biology models. The prevalence of sloppiness highlights the power of collective fits and suggests that modelers should focus on predictions rather than on parameters.},
  langid = {english},
  pmcid = {PMC2000971},
  pmid = {17922568},
  keywords = {Algorithms,Computer Simulation,Half-Life,Meta-Analysis as Topic,Metabolic Networks and Pathways,{Models, Biological},{Models, Statistical},Monte Carlo Method,Nonlinear Dynamics,Probability,read,Sensitivity and Specificity,Systems Biology}
}

@phdthesis{hadlock_quantile-parameterized_2017,
  type = {Thesis},
  title = {Quantile-Parameterized Methods for Quantifying Uncertainty in Decision Analysis},
  author = {Hadlock, Christopher Campbell},
  year = {2017},
  month = may,
  doi = {10.15781/T2F18SX41},
  urldate = {2022-01-16},
  abstract = {In decision analysis, analysts must often encode an expert's uncertainty of a quantity (e.g., the volume of oil reserves in a proven, but undeveloped oil field) using a probability distribution. This is commonly done by eliciting a triplet of low-base-high percentile assessments, such as the \{10th, 50th, 90th\} percentiles, from the expert, and then fitting a probability distribution from a well-known family (e.g., lognormal) to the assessed quantile-probability (or QP) pairs. However, curve fitting often requires non-linear, non-convex optimization over a distribution parameter space, and the fitted distribution often never honors the assessed QP pairs \textendash{} reducing both the fidelity of the model, and trust in the analysis. The development of quantile-parameterized distributions (or QPDs), distributions that are parameterized by, and thus precisely honor the assessed QP pairs, is a very important yet nascent topic in decision analysis, and contributions in the literature are sparse. This dissertation extends existing work on QPDs by strategically developing a new smooth probability distribution system (known as J-QPD) that is parameterized by (and honors) assessed QP pairs. J-QPD also honors various natural support regimes \textendash{} for example: bounded (e.g., fractional uncertainties, such as market shares, are necessarily bounded between zero and one); semi-bounded (e.g., volume of oil reserves is necessarily non-negative, but may have no well-defined upper bound); etc. We then show that J-QPD is maximally-feasible, highly flexible, and approximates the shapes of a vast array of commonly-named distributions (e.g., normal, lognormal, beta, etc.) with potent accuracy, using a single system. This work also presents efficient, high-fidelity methods for capturing dependence between two or more uncertainties by combining J-QPD with modern correlation assessment and modeling techniques. We then provide an application of J-QPD to a famous decision analysis example, demonstrating how J-QPD facilitates rapid Monte Carlo simulation, and how its implementation can aid actual decisions that might otherwise be made wrongly if commonly-used discrete methods are used. We conclude by noting important tradeoffs between J-QPD and existing QPD systems, and offer several extensions for future research, including a first look at designing new discrete distributions using J-QPD.},
  langid = {english},
  keywords = {read}
}

@article{hanea_classical_2018,
  title = {Classical Meets Modern in the {{IDEA}} Protocol for Structured Expert Judgement},
  author = {Hanea, A.M. and McBride, M.F. and Burgman, M.A. and Wintle, B.C.},
  year = {2018},
  month = apr,
  journal = {Journal of Risk Research},
  volume = {21},
  number = {4},
  pages = {417--433},
  publisher = {{Routledge}},
  issn = {1366-9877},
  doi = {10.1080/13669877.2016.1215346},
  urldate = {2022-06-23},
  abstract = {Expert judgement is pervasive in all forms of risk analysis, yet the development of tools to deal with such judgements in a repeatable and transparent fashion is relatively recent. This work outlines new findings related to an approach to expert elicitation termed the IDEA protocol. IDEA combines psychologically robust interactions among experts with mathematical aggregation of individual estimates. In particular, this research explores whether communication among experts adversely effects the reliability of group estimates. Using data from estimates of the outcomes of geopolitical events, we find that loss of independence is relatively modest and it is compensated by improvements in group accuracy.},
  keywords = {combining experts' opinion,eliciting event probabilities,experts' dependence,mathematical aggregation,read,structured expert judgement}
}

@inproceedings{hartmann_flexible_2020,
  title = {Flexible Prior Elicitation via the Prior Predictive Distribution},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Hartmann, Marcelo and Agiashvili, Georgi and B{\"u}rkner, Paul and Klami, Arto},
  year = {2020},
  month = aug,
  pages = {1129--1138},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-01-16},
  abstract = {The prior distribution for the unknown model parameters plays a crucial role in the process of statistical inference based on Bayesian methods. However, specifying suitable priors is often difficult even when detailed prior knowledge is available in principle. The challenge is to express quantitative information in the form of a probability distribution. Prior elicitation addresses this question by extracting subjective information from an expert and transforming it into a valid prior. Most existing methods, however, require information to be provided on the unobservable parameters, whose effect on the data generating process is often complicated and hard to understand. We propose an alternative approach that only requires knowledge about the observable outcomes - knowledge which is often much easier for experts to provide. Building upon a principled statistical framework, our approach utilizes the prior predictive distribution implied by the model to automatically transform experts judgements about plausible outcome values to suitable priors on the parameters. We also provide computational strategies to perform inference and guidelines to facilitate practical use.},
  langid = {english},
  keywords = {read}
}

@article{hassen_approximate_2021,
  title = {Approximate {{Bayesian Optimisation}} for {{Neural Networks}}},
  author = {Hassen, Nadhir and Rish, Irina},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.12461 [cs, math, stat]},
  eprint = {2108.12461},
  primaryclass = {cs, math, stat},
  urldate = {2022-04-04},
  abstract = {A body of work has been done to automate machine learning algorithm to highlight the importance of model choice. Automating the process of choosing the best forecasting model and its corresponding parameters can result to improve a wide range of real-world applications. Bayesian optimisation (BO) uses a blackbox optimisation methods to propose solutions according to an exploration-exploitation trade-off criterion through acquisition functions. BO framework imposes two key ingredients: a probabilistic surrogate model that consist of prior belief of the unknown objective function(data-dependant) and an objective function that describes how optimal is the model-fit. Choosing the best model and its associated hyperparameters can be very expensive, and is typically fit using Gaussian processes (GPs) and at some extends applying approximate inference due its intractability. However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations. In addition, most real-dataset are non-stationary which make idealistic assumptions on surrogate models. The necessity to solve the analytical tractability and the computational feasibility in a stochastic fashion enables to ensure the efficiency and the applicability of Bayesian optimisation. In this paper we explore the use of neural networks as an alternative to GPs to model distributions over functions, we provide a link between density-ratio estimation and class probability estimation based on approximate inference, this reformulation provides algorithm efficiency and tractability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@book{hastie_generalized_1999,
  ids = {hastie_generalized_1999-1},
  title = {Generalized Additive Models},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1999},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton, Fla}},
  isbn = {978-0-412-34390-2},
  langid = {english},
  lccn = {QA278.2 .H39 1999},
  keywords = {Linear models (Statistics),Regression analysis,Smoothing (Statistics)}
}

@article{held_reverse-bayes_2022,
  title = {Reverse-{{Bayes}} Methods for Evidence Assessment and Research Synthesis},
  author = {Held, Leonhard and Matthews, Robert and Ott, Manuela and Pawel, Samuel},
  year = {2022},
  journal = {Research Synthesis Methods},
  volume = {13},
  number = {3},
  pages = {295--314},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1538},
  urldate = {2022-09-23},
  abstract = {It is now widely accepted that the standard inferential toolkit used by the scientific research community\textemdash null-hypothesis significance testing (NHST)\textemdash is not fit for purpose. Yet despite the threat posed to the scientific enterprise, there is no agreement concerning alternative approaches for evidence assessment. This lack of consensus reflects long-standing issues concerning Bayesian methods, the principal alternative to NHST. We report on recent work that builds on an approach to inference put forward over 70 years ago to address the well-known ``Problem of Priors'' in Bayesian analysis, by reversing the conventional prior-likelihood-posterior (``forward'') use of Bayes' theorem. Such Reverse-Bayes analysis allows priors to be deduced from the likelihood by requiring that the posterior achieve a specified level of credibility. We summarise the technical underpinning of this approach, and show how it opens up new approaches to common inferential challenges, such as assessing the credibility of scientific findings, setting them in appropriate context, estimating the probability of successful replications, and extracting more insight from NHST while reducing the risk of misinterpretation. We argue that Reverse-Bayes methods have a key role to play in making Bayesian methods more accessible and attractive for evidence assessment and research synthesis. As a running example we consider a recently published meta-analysis from several randomised controlled trials (RCTs) investigating the association between corticosteroids and mortality in hospitalised patients with COVID-19.},
  langid = {english},
  keywords = {Analysis of Credibility,Bayes factor,false positive risk,meta-analysis,prior-data conflict,Reverse-Bayes}
}

@phdthesis{hem_robustifying_2021,
  title = {Robustifying {{Bayesian}} Hierarchical Models Using Intuitive Prior Elicitation},
  author = {Hem, Ingeborg Gullikstad},
  year = {2021},
  month = jun,
  langid = {english},
  school = {Norwegian University of Science and Technology},
  keywords = {read}
}

@inproceedings{horn_first_2017,
  title = {First {{Investigations}} on {{Noisy Model-Based Multi-objective Optimization}}},
  booktitle = {Evolutionary {{Multi-Criterion Optimization}}},
  author = {Horn, Daniel and Dagge, Melanie and Sun, Xudong and Bischl, Bernd},
  editor = {Trautmann, Heike and Rudolph, G{\"u}nter and Klamroth, Kathrin and Sch{\"u}tze, Oliver and Wiecek, Margaret and Jin, Yaochu and Grimme, Christian},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {298--313},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-54157-0_21},
  abstract = {In many real-world applications concerning multi-objective optimization, the true objective functions are not observable. Instead, only noisy observations are available. In recent years, the interest in the effect of such noise in evolutionary multi-objective optimization (EMO) has increased and many specialized algorithms have been proposed. However, evolutionary algorithms are not suitable if the evaluation of the objectives is expensive and only a small budget is available. One popular solution is to use model-based multi-objective optimization (MBMO) techniques. In this paper, we present a first investigation on noisy MBMO. For this purpose we collect several noise handling strategies from the field of EMO and adapt them for MBMO algorithms. We compare the performance of those strategies in two benchmark situations: Firstly, we perform a purely artificial benchmark using homogeneous Gaussian noise. Secondly, we choose a setting from the field of machine learning, where the structure of the underlying noise is unknown.},
  isbn = {978-3-319-54157-0},
  langid = {english},
  keywords = {Bayesian optimization,Machine learning,Model-based optimization,Multi-objective optimization,Noisy optimization}
}

@article{ibrahim_bayesian_2001-1,
  ids = {ibrahim_bayesian_2001-2},
  title = {Bayesian Semiparametric Models for Survival Data with a Cure Fraction},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Sinha, Debajyoti},
  year = {2001},
  journal = {Biometrics. Journal of the International Biometric Society},
  volume = {57},
  number = {2},
  pages = {383--388},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2001.00383.x},
  urldate = {2022-06-08},
  abstract = {Summary. We propose methods for Bayesian inference for a new class of semiparametric survival models with a cure fraction. Specifically, we propose a semiparametric cure rate model with a smoothing parameter that controls the degree of parametricity in the right tail of the survival distribution. We show that such a parameter is crucial for these kinds of models and can have an impact on the posterior estimates. Several novel properties of the proposed model are derived. In addition, we propose a class of improper noninformative priors based on this model and examine the properties of the implied posterior. Also, a class of informative priors based on historical data is proposed and its theoretical properties are investigated. A case study involving a melanoma clinical trial is discussed in detail to demonstrate the proposed methodology.},
  langid = {english},
  keywords = {Cure rate model,Gibbs sampling,Historical data,Latent variables,Piecewise exponential,Posterior distribution,Semiparametric model,Smoothing parameter}
}

@article{ibrahim_power_2015,
  title = {The Power Prior: {{Theory}} and Applications},
  shorttitle = {The Power Prior},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
  year = {2015},
  journal = {Statistics in Medicine},
  volume = {34},
  number = {28},
  pages = {3724--3749},
  issn = {1097-0258},
  doi = {10.1002/sim.6728},
  urldate = {2022-06-27},
  abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A-to-Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Frequentist properties of power priors in posterior inference are established, and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  pmcid = {PMC4626399},
  pmid = {26346180},
  keywords = {Bayesian design,borrowing,clinical trials,discounting,historical data,informative prior}
}

@article{ibrahim_predictive_1994,
  title = {A Predictive Approach to the Analysis of Designed Experiments},
  author = {Ibrahim, Joseph G. and Laud, Purushottam W.},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  pages = {309--319},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291227},
  urldate = {2022-01-11},
  abstract = {Viewing the analysis of designed experiments as a model selection problem, we introduce the use of a predictive Bayesian criterion in this context based on the predictive density of a replicate experiment (PDRE). A calibration of the criterion is provided to assist in the model choice. The relationships of the proposed criterion to other prevalent criteria, such as AIC, BIC, and Mallows's C\textsubscript{p}, are given. An information theoretic criterion based on the PDRE's of two competing models is also introduced and compared with the usual F statistic for two nested models. Examples are given to illustrate the proposed methodology.},
  jstor = {2291227},
  keywords = {read}
}

@article{ibrahim_properties_1997,
  title = {On Properties of Predictive Priors in Linear Models},
  author = {Ibrahim, Joseph G.},
  year = {1997},
  month = nov,
  journal = {The American Statistician},
  volume = {51},
  number = {4},
  pages = {333--337},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1997.10474408},
  urldate = {2022-07-14},
  abstract = {Utilizing the notion of matching predictives as in Berger and Pericchi, we show that for the conjugate family of prior distributions in the normal linear model, the symmetric Kullback-Leibler divergence between two particular predictive densities is minimized when the prior hyperparameters are taken to be those corresponding to the predictive priors proposed in Ibrahim and Laud and Laud and Ibrahim. The main application for this result is for Bayesian variable selection.},
  keywords = {Kullback-Leibler divergence,Matching predictives,Predictive distribution,Prior distribution,Variable selection}
}

@misc{iskauskas_hmer_2022,
  title = {{{{\textsc{hmer}}}}: History {{Matching}} and {{Emulation Package}}},
  shorttitle = {Hmer},
  author = {Iskauskas, Andrew},
  year = {2022},
  month = may,
  urldate = {2022-06-13},
  abstract = {A set of objects and functions for Bayes Linear emulation and history matching. Core functionality includes automated training of emulators to data, diagnostic functions to ensure suitability, and a variety of proposal methods for generating 'waves' of points. For details on the mathematical background, there are many papers available on the topic (see references attached to function help files); for details of the functions in this package, consult the manual or help files.},
  copyright = {MIT + file LICENSE},
  keywords = {read}
}

@article{jarocinski_priors_2019,
  title = {Priors about Observables in Vector Autoregressions},
  author = {Jaroci{\'n}ski, Marek and Marcet, Albert},
  year = {2019},
  month = apr,
  journal = {Journal of Econometrics},
  volume = {209},
  number = {2},
  pages = {238--255},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2018.12.023},
  urldate = {2022-01-16},
  abstract = {Standard practice in Bayesian VARs is to formulate priors on the autoregressive parameters, but economists and policy makers actually have priors about the behavior of observable variables. We show how to translate the prior on observables into a prior on parameters using strict probability theory principles, a posterior can then be formed with standard procedures. We state the inverse problem to be solved and we propose a numerical algorithm that works well in practical situations. We prove equivalence to a fixed point formulation and a convergence theorem for the algorithm. We use this framework in two well known applications in the VAR literature, we show how priors on observables can address some weaknesses of standard priors, serving as a cross check and an alternative formulation.},
  langid = {english},
  keywords = {read}
}

@article{johnson_methods_2010,
  title = {Methods to Elicit Beliefs for {{Bayesian}} Priors: {{A}} Systematic Review},
  shorttitle = {Methods to Elicit Beliefs for {{Bayesian}} Priors},
  author = {Johnson, Sindhu R. and Tomlinson, George A. and Hawker, Gillian A. and Granton, John T. and Feldman, Brian M.},
  year = {2010},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {4},
  pages = {355--369},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2009.06.003},
  urldate = {2022-06-22},
  abstract = {Objective Bayesian analysis can incorporate clinicians' beliefs about treatment effectiveness into models that estimate treatment effects. Many elicitation methods are available, but it is unclear if any confer advantages based on principles of measurement science. We review belief-elicitation methods for Bayesian analysis and determine if any of them had an incremental value over the others based on its validity, reliability, and responsiveness. Study Design and Setting A systematic review was performed. MEDLINE, EMBASE, CINAHL, Health and Psychosocial Instruments, Current Index to Statistics, MathSciNet, and Zentralblatt Math were searched using the terms (prior OR prior probability distribution) AND (beliefs OR elicitation) AND (Bayes OR Bayesian). Studies were evaluated on: design, question stem, response options, analysis, consideration of validity, reliability, and responsiveness. Results We identified 33 studies describing methods for elicitation in a Bayesian context. Elicitation occurred in cross-sectional studies (n=30, 89\%), to derive point estimates with individual-level variation (n=19; 58\%). Although 64\% (n=21) considered validity, 24\% (n=8) reliability, 12\% (n=4) responsiveness of the elicitation methods, only 12\% (n=4) formally tested validity, 6\% (n=2) tested reliability, and none tested responsiveness. Conclusions We have summarized methods of belief elicitation for Bayesian priors. The validity, reliability, and responsiveness of elicitation methods have been infrequently evaluated. Until comparative studies are performed, strategies to reduce the effects of bias on the elicitation should be used.},
  langid = {english},
  keywords = {Bayesian,Belief elicitation,Bias,Priors,read,Reliability,Validity}
}

@misc{johnson_nlopt_2014,
  title = {The {{{\textsc{NLopt}}}} Nonlinear-Optimization Package},
  author = {Johnson, Steven G.},
  year = {2014}
}

@article{johnson_valid_2010,
  title = {A Valid and Reliable Belief Elicitation Method for {{Bayesian}} Priors},
  author = {Johnson, Sindhu R. and Tomlinson, George A. and Hawker, Gillian A. and Granton, John T. and Grosbein, Haddas A. and Feldman, Brian M.},
  year = {2010},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {4},
  pages = {370--383},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2009.08.005},
  urldate = {2022-06-22},
  abstract = {Objective Bayesian inference has the advantage of formally incorporating prior beliefs about the effect of an intervention into analyses of treatment effect through the use of prior probability distributions or ``priors.'' Multiple methods to elicit beliefs from experts for inclusion in a Bayesian study have been used; however, the measurement properties of these methods have been infrequently evaluated. The objectives of this study were to evaluate the feasibility, validity, and reliability of a belief elicitation method for Bayesian priors. Study Design and Setting A single-center, cross-sectional study using a sample of academic specialists who treat pulmonary hypertension patients was conducted to test the feasibility, face and construct validity, and reliability of a belief elicitation method. Using this method, participants expressed the probability of 3-year survival with and without warfarin. Applying adhesive dots or ``chips,'' each representing 5\% probability, in ``bins'' on a line, participants expressed their uncertainty and weight of belief about the effect of warfarin on 3-year survival. Results Of the 12 participants, 11 (92\%) reported that the belief elicitation method had face validity, 10 (83\%) found the questions clear, and 11 (92\%) found the response option easy to use. The median time to completion was 10 minutes (5\textendash 15 minutes). Internal validity testing found moderate agreement (weighted kappa=0.54\textendash 0.57). The intraclass correlation coefficient for test\textendash retest reliability was 0.93. Conclusion This method of belief elicitation for Bayesian priors is feasible, valid, and reliable. It can be considered for application in Bayesian clinical studies.},
  langid = {english},
  keywords = {Bayesian,Belief elicitation,Priors,Pulmonary hypertension,read,Reliability,Validity}
}

@article{jones_efficient_1998,
  title = {Efficient {{Global Optimization}} of {{Expensive Black-Box Functions}}},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  month = dec,
  journal = {Journal of Global Optimization},
  volume = {13},
  number = {4},
  pages = {455--492},
  issn = {1573-2916},
  doi = {10.1023/A:1008306431147},
  urldate = {2022-02-27},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  langid = {english}
}

@article{kadane_experiences_1998,
  ids = {kadane_experiences_1998-1},
  title = {Experiences in Elicitation},
  author = {Kadane, Joseph and Wolfson, Lara J.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  volume = {47},
  number = {1},
  pages = {3--19},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1467-9884},
  doi = {10.1111/1467-9884.00113},
  urldate = {2021-09-23},
  abstract = {Elicitation of expert opinion is becoming increasingly important in the elicitation of prior distributions. In this paper, the psychology of elicitation and the currently available methods are briefly reviewed, but the primary discussion is on the distinction between `general'elicitation methods for a class of problems and `application-specific' methods which are useful only once. Examples of both types of elicitation are given, along with a discussion about general versus application-specific methods, and predictive versus structural elicitation},
  langid = {english},
  keywords = {Demography,Expert opinion,Prior probability,Probability assessment,read,Time series}
}

@phdthesis{kaelo_population_2005,
  title = {Some {{Population Set-Based Methods}} for {{Unconstrained Global Optimization}}},
  author = {Kaelo, Professor},
  year = {2005},
  address = {{Johannesburg}},
  langid = {english},
  school = {University of the Witwatersrand}
}

@article{kaelo_variants_2006,
  title = {Some Variants of the Controlled Random Search Algorithm for Global Optimization},
  author = {Kaelo, P. and Ali, M. M.},
  year = {2006},
  month = dec,
  journal = {Journal of Optimization Theory and Applications},
  volume = {130},
  number = {2},
  pages = {253--264},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/s10957-006-9101-0},
  urldate = {2022-04-19},
  langid = {english}
}

@article{kallioinen_detecting_2021,
  title = {Detecting and Diagnosing Prior and Likelihood Sensitivity with Power-Scaling},
  author = {Kallioinen, Noa and Paananen, Topi and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.14054 [stat]},
  eprint = {2107.14054},
  primaryclass = {stat},
  urldate = {2022-03-29},
  abstract = {Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach that is applicable to a wide range of models, based on power-scaling perturbations. We suggest a diagnostic based on this that can indicate the presence of prior-data conflict or likelihood noninformativity. The approach can be easily included in Bayesian workflows with minimal work by the model builder. We present the implementation of the approach in our new R package priorsense and demonstrate the workflow on case studies of real data.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Methodology}
}

@article{kass_selection_1996,
  title = {The {{Selection}} of {{Prior Distributions}} by {{Formal Rules}}},
  author = {Kass, Robert E. and Wasserman, Larry},
  year = {1996},
  journal = {Journal of the American Statistical Association},
  volume = {91},
  number = {435},
  pages = {1343--1370},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291752},
  urldate = {2022-06-23},
  abstract = {Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet in practice, most Bayesian analyses are performed with so-called "noninformative" priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys's rules and discuss the evolution of his viewpoint about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly: When sample sizes are small (relative to the number of parameters being estimated), it is dangerous to put faith in any "default" solution; but when asymptotics take over, Jeffreys's rules and their variants remain reasonable choices. We also provide an annotated bibliography.},
  jstor = {2291752},
  keywords = {read}
}

@article{keelin_quantile-parameterized_2011,
  title = {Quantile-Parameterized Distributions},
  author = {Keelin, Thomas W. and Powley, Bradford W.},
  year = {2011},
  month = sep,
  journal = {Decision Analysis},
  volume = {8},
  number = {3},
  pages = {206--219},
  issn = {1545-8490, 1545-8504},
  doi = {10.1287/deca.1110.0213},
  urldate = {2022-01-16},
  langid = {english},
  keywords = {read}
}

@article{kennedy_bayesian_2001,
  title = {Bayesian {{Calibration}} of {{Computer Models}}},
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  year = {2001},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {63},
  number = {3},
  pages = {425--464},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  urldate = {2022-06-13},
  abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
  jstor = {2680584},
  keywords = {read}
}

@book{klein_handbook_2014,
  title = {Handbook of Survival Analysis},
  editor = {Klein, John P. and {van Houwelingen}, Hans and Ibrahim, Joseph George and Scheike, Thomas H.},
  year = {2014},
  series = {Chapman \& {{Hall}} / {{CRC}} Handbooks of Modern Statistical Methods},
  publisher = {{CRC Press}},
  address = {{Boca Raton, Fla.}},
  isbn = {978-1-4665-5567-9 978-1-4665-5566-2},
  langid = {english}
}

@book{klein_survival_2003,
  title = {Survival Analysis: {{Techniques}} for Censored and Truncated Data},
  shorttitle = {Survival Analysis},
  author = {Klein, John P. and Moeschberger, Melvin L.},
  year = {2003},
  series = {Statistics for Biology and Health},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-95399-1},
  langid = {english},
  lccn = {R853.S7 K535 2003},
  keywords = {Survival analysis (Biometry)}
}

@article{kravchuk_hodges-lehmann_2012,
  title = {Hodges-{{Lehmann}} Scale Estimator for {{Cauchy}} Distribution},
  author = {Kravchuk, O. Y. and Pollett, P. K.},
  year = {2012},
  month = oct,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {41},
  number = {20},
  pages = {3621--3632},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610926.2011.563016},
  urldate = {2022-08-28},
  langid = {english}
}

@article{kung_finding_1975,
  title = {On Finding the Maxima of a Set of Vectors},
  author = {Kung, H. T. and Luccio, F. and Preparata, F. P.},
  year = {1975},
  month = oct,
  journal = {Journal of the ACM},
  volume = {22},
  number = {4},
  pages = {469--476},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/321906.321910},
  urldate = {2022-08-10},
  langid = {english}
}

@article{kushner_new_1964,
  title = {A {{New Method}} of {{Locating}} the {{Maximum Point}} of an {{Arbitrary Multipeak Curve}} in the {{Presence}} of {{Noise}}},
  author = {Kushner, H. J.},
  year = {1964},
  month = mar,
  journal = {Journal of Basic Engineering},
  volume = {86},
  number = {1},
  pages = {97--106},
  issn = {0021-9223},
  doi = {10.1115/1.3653121},
  urldate = {2022-09-06},
  abstract = {A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.}
}

@article{kynn_heuristics_2008,
  title = {The '{{Heuristics}} and {{Biases}}' {{Bias}} in {{Expert Elicitation}}},
  author = {Kynn, Mary},
  year = {2008},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {171},
  number = {1},
  pages = {239--264},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0964-1998},
  urldate = {2022-09-21},
  abstract = {In the early 1970s Tversky and Kahneman published a series of papers on 'heuristics and biases' describing human inadequacies in assessing probabilities, culminating in a highly popular article in Science. This seminal research has been heavily cited in many fields, including statistics, as the definitive research on probability assessment. Curiously, although this work was debated at the time and more recent work has largely refuted many of the claims, this apparent heuristics and biases bias in elicitation research has gone unremarked. Over a decade of research into the frequency effect, the importance of framing, and cognitive models more generally, has been almost completely ignored by the statistical literature on expert elicitation. To remedy this situation, this review offers a guide to the psychological research on assessing probabilities, both old and new, and gives concrete guidelines for eliciting expert knowledge.},
  jstor = {30130739}
}

@article{laud_predictive_1995,
  title = {Predictive {{Model Selection}}},
  author = {Laud, Purushottam W. and Ibrahim, Joseph G.},
  year = {1995},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {247--262},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2022-07-14},
  abstract = {We consider the problem of selecting one model from a large class of plausible models. A predictive Bayesian viewpoint is advocated to avoid the specification of prior probabilities for the candidate models and the detailed interpretation of the parameters in each model. Using criteria derived from a certain predictive density and a prior specification that emphasizes the observables, we implement the proposed methodology for three common problems arising in normal linear models: variable subset selection, selection of a transformation of predictor variables and estimation of a parametric variance function. Interpretation of the relative magnitudes of the criterion values for various models is facilitated by a calibration of the criteria. Relationships between the proposed criteria and other well-known criteria are examined.},
  jstor = {2346098}
}

@article{letham_constrained_2019,
  title = {Constrained {{Bayesian Optimization}} with {{Noisy Experiments}}},
  author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
  year = {2019},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {2},
  issn = {1936-0975},
  doi = {10.1214/18-BA1110},
  urldate = {2022-09-06},
  abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasiMonte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
  langid = {english}
}

@article{lewandowski_generating_2009,
  ids = {lewandowski_generating_2009-1},
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  year = {2009},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2009.04.008},
  urldate = {2021-10-05},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276\textendash 294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177\textendash 2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
  langid = {english},
  keywords = {broken-with-latex,Correlation matrix,Dependence vines,Onion method,Partial correlation}
}

@article{liberti_introduction_2008,
  title = {Introduction to Global Optimization},
  author = {Liberti, Leo},
  year = {2008},
  month = oct,
  pages = {43},
  abstract = {Accurate modelling of real-world problems often requires nonconvex terms to be introduced in the model, either in the objective function or in the constraints. Nonconvex programming is one of the hardest fields of optimization, presenting many challenges in both practical and theoretical aspects. The presence of multiple local minima calls for the application of global optimization techniques. This paper is a mini-course about global optimization techniques in nonconvex programming; it deals with some theoretical aspects of nonlinear programming as well as with some of the current stateof-the-art algorithms in global optimization. The syllabus is as follows. Some examples of Nonlinear Programming Problems (NLPs). General description of two-phase algorithms. Local optimization of NLPs: derivation of KKT conditions. Short notes about stochastic global multistart algorithms with a concrete example (SobolOpt). In-depth study of a deterministic spatial Branch-and-Bound algorithm, and convex relaxation of an NLP. Latest advances in bilinear programming: the theory of reduction constraints.},
  langid = {english}
}

@incollection{low_choy_priors_2012,
  title = {Priors: {{Silent}} or Active Partners of {{Bayesian}} Inference?},
  shorttitle = {Priors},
  booktitle = {Case {{Studies}} in {{Bayesian Statistical Modelling}} and {{Analysis}}},
  author = {Low Choy, Samantha},
  editor = {Alston, Clair L. and Mengersen, Kerrie L. and Pettitt, Anthony N.},
  year = {2012},
  pages = {30--65},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118394472.ch3},
  urldate = {2022-06-21},
  abstract = {This chapter contains sections titled: Priors in the very beginning Methodology I: Priors defined by mathematical criteria Methodology II: Modelling informative priors Case studies Discussion Acknowledgements References},
  chapter = {3},
  isbn = {978-1-118-39447-2},
  langid = {english},
  keywords = {Bayesian analyses in practice,conditionally conjugate for a parameter,formulation of priors,from `objective' to `subjective',in Bayesian inference,in problem-solving/inference,Jeffreys' priors and maximum likelihood,methodology for prior models,modelling informative priors,multi-faceted role of priors,one of two main goals,prior,prior and communication of results,prior formulation,priors,read,silent or active partners of Bayesian inference,success of Bayesian inference,within Bayesian cycle of learning}
}

@article{maechler_accurately_2012,
  title = {Accurately Computing Log(1 - {{Exp}}(-|a|)) Assessed by the {{{\textsc{Rmpfr}}}} Package},
  author = {Maechler, Martin},
  year = {2012},
  pages = {9},
  langid = {english},
  keywords = {read}
}

@misc{maechler_rmpfr_2021,
  title = {{\textsc{Rmpfr}}: {{R MPFR}} - Multiple Precision Floating-Point Reliable},
  shorttitle = {Rmpfr},
  author = {Maechler, Martin and Heiberger, Richard M. and Nash, John C. and Borchers, Hans W.},
  year = {2021},
  month = oct,
  urldate = {2022-03-28},
  abstract = {Arithmetic (via S4 classes and methods) for arbitrary precision floating point numbers, including transcendental ("special") functions. To this end, the package interfaces to the 'LGPL' licensed 'MPFR' (Multiple Precision Floating-Point Reliable) Library which itself is based on the 'GMP' (GNU Multiple Precision) Library.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {NumericalMathematics,read}
}

@book{maller_survival_1996,
  title = {Survival Analysis with Long-Term Survivors},
  author = {Maller, Ross A. and Zhou, Xian},
  year = {1996},
  series = {Wiley Series in Probability and Statistics},
  publisher = {{Wiley}},
  address = {{Chichester ; New York}},
  isbn = {978-0-471-96201-4},
  lccn = {R853.S7 M355 1996},
  keywords = {Immunity,Survival Analysis,Survival analysis (Biometry),Survivors,Time Factors}
}

@article{manderson_combining_2022,
  ids = {manderson_combining_2021,manderson_combining_2021-1,manderson_combining_2022-2,manderson_combining_2022-3},
  title = {Combining Chains of {{Bayesian}} Models with {{Markov}} Melding},
  author = {Manderson, Andrew A. and Goudie, Robert J. B.},
  year = {2022},
  month = aug,
  journal = {Bayesian Analysis},
  eprint = {2111.11566},
  primaryclass = {stat},
  pages = {1--34},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/22-BA1327},
  urldate = {2022-09-13},
  abstract = {A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data sets. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data sets are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings.},
  archiveprefix = {arxiv},
  keywords = {Bayesian graphical models,combining models,integrated population model,Markov melding,model/data integration,multi-stage estimation,Statistics - Applications,Statistics - Methodology}
}

@misc{manderson_Translating_2023,
  title = {Translating Predictive Distributions into Informative Priors},
  author = {Manderson, Andrew A. and Goudie, Robert J. B.},
  year = {2023},
  month = jun,
  number = {arXiv:2303.08528},
  eprint = {2303.08528},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08528},
  urldate = {2023-09-08},
  abstract = {When complex Bayesian models exhibit implausible behaviour, one solution is to assemble available information into an informative prior. Challenges arise as prior information is often only available for the observable quantity, or some model-derived marginal quantity, rather than directly pertaining to the natural parameters in our model. We propose a method for translating available prior information, in the form of an elicited distribution for the observable or model-derived marginal quantity, into an informative joint prior. Our approach proceeds given a parametric class of prior distributions with as yet undetermined hyperparameters, and minimises the difference between the supplied elicited distribution and corresponding prior predictive distribution. We employ a global, multi-stage Bayesian optimisation procedure to locate optimal values for the hyperparameters. Three examples illustrate our approach: a cure-fraction survival model, where censoring implies that the observable quantity is a priori a mixed discrete/continuous quantity; a setting in which prior information pertains to \$R\^\{2\}\$ -- a model-derived quantity; and a nonlinear regression model.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/andrew-agreena/Zotero/storage/3REBS6P7/Manderson and Goudie - 2023 - Translating predictive distributions into informat.pdf}
}

@article{margossian_efficient_2021,
  title = {Efficient {{Automatic Differentiation}} of {{Implicit Functions}}},
  author = {Margossian, Charles and Betancourt, Michael},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.14217 [stat]},
  eprint = {2112.14217},
  primaryclass = {stat},
  urldate = {2022-01-14},
  abstract = {Derivative-based algorithms are ubiquitous in statistics, machine learning, and applied mathematics. Automatic differentiation offers an algorithmic way to efficiently evaluate these derivatives from computer programs that execute relevant functions. Implementing automatic differentiation for programs that incorporate implicit functions, such as the solution to an algebraic or differential equation, however, requires particular care. Contemporary applications typically appeal to either the application of the implicit function theorem or, in certain circumstances, specialized adjoint methods. In this paper we show that both of these approaches can be generalized to any implicit function, although the generalized adjoint method is typically more effective for automatic differentiation. To showcase the relative advantages and limitations of the two methods we demonstrate their application on a suite of common implicit functions.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation}
}

@article{marin_approximate_2012,
  title = {Approximate {{Bayesian}} Computational Methods},
  author = {Marin, Jean-Michel and Pudlo, Pierre and Robert, Christian P. and Ryder, Robin J.},
  year = {2012},
  month = nov,
  journal = {Statistics and Computing},
  volume = {22},
  number = {6},
  pages = {1167--1180},
  issn = {1573-1375},
  doi = {10.1007/s11222-011-9288-2},
  urldate = {2022-06-24},
  abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years.},
  langid = {english},
  keywords = {ABC methodology,Bayesian model choice,Bayesian statistics,DIYABC,Likelihood-free methods}
}

@article{mikkola_prior_2021,
  title = {Prior Knowledge Elicitation: {{The}} Past, Present, and Future},
  shorttitle = {Prior Knowledge Elicitation},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and B{\"u}rkner, Paul-Christian and Klami, Arto},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.01380 [stat]},
  eprint = {2112.01380},
  primaryclass = {stat},
  urldate = {2021-12-05},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Methodology}
}

@article{mikkola_Prior_2023,
  title = {Prior Knowledge Elicitation: {{The}} Past, Present, and Future},
  shorttitle = {Prior Knowledge Elicitation},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and B{\"u}rkner, Paul-Christian and Klami, Arto},
  year = {2023},
  month = jan,
  journal = {Bayesian Analysis},
  pages = {1--33},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/23-BA1381},
  urldate = {2023-06-25},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. In principle, prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyse the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  keywords = {Bayesian workflow,domain knowledge,informative prior,prior distribution,prior elicitation}
}

@article{minka_divergence_2005,
  title = {Divergence Measures and Message Passing},
  author = {Minka, Thomas},
  year = {2005},
  pages = {17},
  abstract = {This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
  langid = {english}
}

@incollection{mockus_bayesian_1975,
  title = {On {{Bayesian Methods}} for {{Seeking}} the {{Extremum}}},
  booktitle = {Optimization {{Techniques IFIP Technical Conference}}: {{Novosibirsk}}, {{July}} 1\textendash 7, 1974},
  author = {Mo{\v c}kus, J.},
  editor = {Marchuk, G. I.},
  year = {1975},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {400--404},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-38527-2_55},
  urldate = {2022-09-07},
  abstract = {Many well known methods for seeking the extremum had been developed on the basis of quadratic approximation.},
  isbn = {978-3-662-38527-2},
  langid = {english}
}

@article{mohamed_monte_2020,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  month = sep,
  journal = {arXiv:1906.10652 [cs, math, stat]},
  eprint = {1906.10652},
  primaryclass = {cs, math, stat},
  urldate = {2022-01-24},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies\textendash the pathwise, score function, and measure-valued gradient estimators\textendash exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,uncited-but-relevant}
}

@article{mullen_continuous_2014,
  title = {Continuous Global Optimization in {{R}}},
  author = {Mullen, Katharine M.},
  year = {2014},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {60},
  pages = {1--45},
  issn = {1548-7660},
  doi = {10.18637/jss.v060.i06},
  urldate = {2022-06-15},
  abstract = {This article surveys currently available implementations in R for continuous global optimization problems. A new R package globalOptTests is presented that provides a set of standard test problems for continuous global optimization based on C functions by Ali, Khompatraporn, and Zabinsky (2005). 48 of the objective functions contained in the package are used in empirical comparison of 18 R implementations in terms of the quality of the solutions found and speed.},
  copyright = {Copyright (c) 2012 Katharine M. Mullen},
  langid = {english}
}

@inproceedings{nalisnick_predictive_2021,
  title = {Predictive {{Complexity Priors}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Nalisnick, Eric and Gordon, Jonathan and {Hernandez-Lobato}, Jose Miguel},
  year = {2021},
  month = mar,
  pages = {694--702},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-02-15},
  abstract = {Specifying a Bayesian prior is notoriously difficult for complex models such as neural networks. Reasoning about parameters is made challenging by the high-dimensionality and over-parameterization of the space. Priors that seem benign and uninformative can have unintuitive and detrimental effects on a model's predictions. For this reason, we propose predictive complexity priors: a functional prior that is defined by comparing the model's predictions to those of a reference model. Although originally defined on the model outputs, we transfer the prior to the model parameters via a change of variables. The traditional Bayesian workflow can then proceed as usual. We apply our predictive complexity prior to high-dimensional regression, reasoning over neural network depth, and sharing of statistical strength for few-shot learning.},
  langid = {english},
  keywords = {read}
}

@article{nelder_simplex_1965,
  title = {A {{Simplex Method}} for {{Function Minimization}}},
  author = {Nelder, J. A. and Mead, R.},
  year = {1965},
  month = jan,
  journal = {The Computer Journal},
  volume = {7},
  number = {4},
  pages = {308--313},
  issn = {0010-4620},
  doi = {10.1093/comjnl/7.4.308},
  urldate = {2022-08-11},
  abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}
}

@article{nesterov_random_2017,
  title = {Random Gradient-Free Minimization of Convex Functions},
  author = {Nesterov, Yurii and Spokoiny, Vladimir},
  year = {2017},
  month = apr,
  journal = {Foundations of Computational Mathematics},
  volume = {17},
  number = {2},
  pages = {527--566},
  issn = {1615-3375, 1615-3383},
  doi = {10.1007/s10208-015-9296-2},
  urldate = {2021-07-23},
  abstract = {In this paper, we prove the complexity bounds for methods of Convex Optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true both for nonsmooth and smooth problems. For the later class, we present also an accelerated scheme with the expected rate of convergence O(n2/k2), where k is the iteration counter. For Stochastic Optimization, we propose a zero-order scheme and justify its expected rate of convergence O(n/k1/2). We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, both for smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.},
  langid = {english}
}

@article{nicholson_interoperability_2022,
  title = {Interoperability of {{Statistical Models}} in {{Pandemic Preparedness}}: {{Principles}} and {{Reality}}},
  shorttitle = {Interoperability of {{Statistical Models}} in {{Pandemic Preparedness}}},
  author = {Nicholson, George and Blangiardo, Marta and Briers, Mark and Diggle, Peter J. and Fjelde, Tor Erlend and Ge, Hong and Goudie, Robert J. B. and Jersakova, Radka and King, Ruairidh E. and Lehmann, Brieuc C. L. and Mallon, Ann-Marie and Padellini, Tullia and Teh, Yee Whye and Holmes, Chris and Richardson, Sylvia},
  year = {2022},
  month = may,
  journal = {Statistical Science},
  volume = {37},
  number = {2},
  pages = {183--206},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/22-STS854},
  urldate = {2022-07-27},
  abstract = {We present interoperability as a guiding framework for statistical modelling to assist policy makers asking multiple questions using diverse datasets in the face of an evolving pandemic response. Interoperability provides an important set of principles for future pandemic preparedness, through the joint design and deployment of adaptable systems of statistical models for disease surveillance using probabilistic reasoning. We illustrate this through case studies for inferring and characterising spatial-temporal prevalence and reproduction numbers of SARS-CoV-2 infections in England.},
  keywords = {Bayesian graphical models,Bayesian melding,Covid-19,evidence synthesis,interoperability,modularization,multi-source inference}
}

@article{nott_approximation_2018-1,
  title = {Approximation of {{Bayesian}} Predictive {{P-values}} with Regression {{ABC}}},
  author = {Nott, David J. and Drovandi, Christopher C. and Mengersen, Kerrie and Evans, Michael},
  year = {2018},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {1},
  issn = {1936-0975},
  doi = {10.1214/16-BA1033},
  urldate = {2021-07-21}
}

@article{oakley_uncertainty_2007,
  title = {Uncertainty in Prior Elicitations: {{A}} Nonparametric Approach},
  shorttitle = {Uncertainty in Prior Elicitations},
  author = {Oakley, J. E. and O'Hagan, A.},
  year = {2007},
  month = feb,
  journal = {Biometrika},
  volume = {94},
  number = {2},
  pages = {427--441},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asm031},
  urldate = {2022-07-27},
  abstract = {A key task in the elicitation of expert knowledge is to construct a distribution from the finite, and usually small, number of statements that have been elicited from the expert. These statements typically specify some quantiles or moments of the distribution. Such statements are not enough to identify the expert's probability distribution uniquely, and the usual approach is to fit some member of a convenient parametric family. There are two clear deficiencies in this solution. First, the expert's beliefs are forced to fit the parametric family. Secondly, no account is then taken of the many other possible distributions that might have fitted the elicited statements equally well. We present a nonparametric approach which tackles both of these deficiencies. We also consider the issue of the imprecision in the elicited probability judgements.},
  langid = {english}
}

@article{ohagan_elicitation_2005,
  title = {Elicitation},
  author = {O'Hagan, Tony},
  year = {2005},
  journal = {Significance},
  volume = {2},
  number = {2},
  pages = {84--86},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2005.00100.x},
  urldate = {2022-05-26},
  abstract = {There are various situations in which it may be important to obtain expert opinion about some unknown quantity or quantities. But it is not enough simply to ask the expert for an estimate of the unknown quantity: we also need to know how far from that estimate the true value might be. Tony O'Hagan describes the process of elicitation: the formulation of the expert's knowledge in the form of a probability distribution.},
  langid = {english}
}

@article{ohagan_expert_2019,
  title = {Expert {{Knowledge Elicitation}}: {{Subjective}} but {{Scientific}}},
  shorttitle = {Expert {{Knowledge Elicitation}}},
  author = {O'Hagan, Anthony},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {69--81},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1518265},
  urldate = {2022-05-26},
  abstract = {Expert opinion and judgment enter into the practice of statistical inference and decision-making in numerous ways. Indeed, there is essentially no aspect of scientific investigation in which judgment is not required. Judgment is necessarily subjective, but should be made as carefully, as objectively, and as scientifically as possible.},
  langid = {english}
}

@book{ohagan_uncertain_2006,
  ids = {ohagan:etal:06},
  title = {Uncertain Judgements: {{Eliciting}} Experts' Probabilities},
  author = {O'Hagan, A. and Buck, C.E. and Daneshkhah, A. and Eiser, J.R. and Garthwaite, P.H. and Jenkinson, D.J. and Oakley, J.E. and Rakow, T.},
  year = {2006},
  publisher = {{Wiley}},
  doi = {10.1002/0470033312},
  isbn = {978-0-470-03330-2},
  keywords = {Bayesian statistical decision theory,Distribution (Probability theory),Mathematical statistics,Probabilities,read,Statistics}
}

@article{oleary_characterising_2015,
  title = {Characterising {{Uncertainty}} in {{Expert Assessments}}: {{Encoding Heavily Skewed Judgements}}},
  shorttitle = {Characterising {{Uncertainty}} in {{Expert Assessments}}},
  author = {O'Leary, Rebecca A. and {Low-Choy}, Samantha and Fisher, Rebecca and Mengersen, Kerrie and Caley, M. Julian},
  year = {2015},
  month = oct,
  journal = {PLoS ONE},
  volume = {10},
  number = {10},
  pages = {e0141697},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0141697},
  urldate = {2022-07-27},
  abstract = {When limited or no observed data are available, it is often useful to obtain expert knowledge about parameters of interest, including point estimates and the uncertainty around these values. However, it is vital to elicit this information appropriately in order to obtain valid estimates. This is particularly important when the experts' uncertainty about these estimates is strongly skewed, for instance when their best estimate is the same as the lowest value they consider possible. Also this is important when interest is in the aggregation of elicited values. In this paper, we compare alternative distributions for describing such estimates. The distributions considered include the lognormal, mirror lognormal, Normal and scaled Beta. The case study presented here involves estimation of the number of species in coral reefs, which requires eliciting counts within broader taxonomic groups, with highly skewed uncertainty estimates. This paper shows substantial gain in using the scaled Beta distribution, compared with Normal or lognormal distributions. We demonstrate that, for this case study on counting species, applying the novel encoding methodology developed in this paper can facilitate the acquisition of more rigorous estimates of (hierarchical) count data and credible bounds. The approach can also be applied to the more general case of enumerating a sampling frame via elicitation.},
  pmcid = {PMC4627781},
  pmid = {26517835}
}

@incollection{oleary_comparison_2009,
  title = {Comparison of Four Expert Elicitation Methods: {{For Bayesian}} Logistic Regression and Classification Trees},
  booktitle = {Interfacing Modelling and Simulation with Mathematical and Computational Sciences: 18th {{IMACS World Congress}}, {{MODSIM09}}, {{Cairns}}, {{Australia}} 13-17 {{July}} 2009 : {{Proceedings}}},
  author = {O'Leary, R. A. and Mengersen, K. and Murray, J. V. and Low Choy, S.},
  editor = {Braddock, R. D. and Newham, Lachlan T. H. and Anderssen, R. S.},
  year = {2009},
  publisher = {{Modelling and Simulation Society of Australia and New Zealand}},
  address = {{Christchurch, N.Z.}},
  isbn = {978-0-9758400-7-8},
  langid = {english}
}

@inproceedings{paszke_pytorch_2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-26},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.}
}

@incollection{peng_Cure_2014,
  title = {Cure Models},
  booktitle = {Handbook of Survival Analysis},
  author = {Peng, Yingwei and Taylor, Jeremy M. G.},
  editor = {Klein, John P. and {van Houwelingen}, Hans C. and Ibrahim, Joseph G. and Scheike, Thomas H.},
  year = {2014},
  series = {Handbooks of {{Modern Statistical Methods}}},
  publisher = {{Routledge Handbooks Online}},
  doi = {10.1201/b16248-8},
  urldate = {2022-06-09},
  abstract = {Handbook of Survival Analysis presents modern techniques and research problems in lifetime data analysis. This area of statistics deals with time-to-event data that is complicated by censoring and the dynamic nature of events occurring in time. With chapters written by leading researchers in the field, the handbook focuses on advances in survival analysis techniques, covering classical and Bayesian approaches. It gives a complete overview of the current status of survival analysis and should inspire further research in the field. Accessible to a wide range of readers, the book provides: An introduction to various areas in survival analysis for graduate students and novices A reference to modern investigations into survival analysis for more established researchers A text or supplement for a second or advanced course in survival analysis A useful guide to statistical methods for analyzing survival data experiments for practicing statisticians},
  isbn = {978-1-4665-5566-2 978-1-4665-5567-9},
  langid = {english}
}

@article{percy_bayesian_2002,
  ids = {percy_bayesian_2002-1,percy_bayesian_2002-2},
  title = {Bayesian Enhanced Strategic Decision Making for Reliability},
  author = {Percy, David F.},
  year = {2002},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {139},
  number = {1},
  pages = {133--145},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(01)00177-1},
  urldate = {2022-01-11},
  abstract = {Successful strategies for maintenance and replacement require good decisions. We might wish to determine how often to perform preventive maintenance, or the optimal time to replace a system. Alternatively, our interest might be in selecting a threshold to adopt for action under condition monitoring, or in choosing suitable warranty schemes for our products. Stochastic reliability models involving unknown parameters are often used to answer such questions. In common with other problems in operational research, some applications of maintenance and replacement are notorious for their lack of data. We present a general review and some new ideas for improving decisions by adopting Bayesian methodology to allow for the uncertainty of model parameters. These include recommendations for specifying suitable prior distributions using predictive elicitation and simple methods for Bayesian simulation. Practical demonstrations are given to illustrate the potential benefits of this approach.},
  langid = {english},
  keywords = {Bayesian analysis,Predictive elicitation,read,Reliability,Simulation}
}

@misc{perepolkin_hybrid_2021,
  type = {Preprint},
  ids = {perepolkin_hybrid_2021-1},
  title = {Hybrid Elicitation and Indirect {{Bayesian}} Inference with Quantile-Parametrized Likelihood},
  author = {Perepolkin, Dmytro and Goodrich, Benjamin and Sahlin, Ullrika},
  year = {2021},
  month = sep,
  publisher = {{Open Science Framework}},
  doi = {10.31219/osf.io/paby6},
  urldate = {2022-01-16},
  abstract = {This paper extends the application of indirect Bayesian inference to probability distributions defined in terms of quantiles of the observable quantities. Quantile-parameterized distributions are characterized by high shape flexibility and interpretability of its parameters, and are therefore useful for elicitation on observables. To encode uncertainty in the quantiles elicited from experts, we propose a Bayesian model based on the metalog distribution and a version of the Dirichlet prior. The resulting ``hybrid'' expert elicitation protocol for characterizing uncertainty in parameters using questions about the observable quantities is discussed and contrasted to parametric and predictive elicitation.},
  langid = {english},
  keywords = {Applied Statistics,bayesian inference,expert knowledge elicitation,Other Statistics and Probability,Physical Sciences and Mathematics,Probability,quantile distributions,quantile-parameterized distributions,read,Statistical Theory,Statistics and Probability}
}

@misc{perepolkin_tenets_2021,
  title = {The Tenets of Indirect Inference in {{Bayesian}} Models},
  author = {Perepolkin, Dmytro and Goodrich, Benjamin and Sahlin, Ullrika},
  year = {2021},
  month = sep,
  publisher = {{OSF Preprints}},
  doi = {10.31219/osf.io/enzgs},
  urldate = {2022-01-31},
  abstract = {This paper extends the application of Bayesian inference to probability distributions defined in terms of its quantile function. We describe the method of *indirect likelihood* to be used in the Bayesian models with sampling distributions which lack an explicit cumulative distribution function. We provide examples and demonstrate the equivalence of the "quantile-based" (indirect) likelihood to the conventional "density-defined" (direct) likelihood. We consider practical aspects of the numerical inversion of quantile function by root-finding required by the indirect likelihood method. In particular, we consider the problem of ensuring the validity of an arbitrary quantile function with the help of Chebyshev polynomials and provide useful tips and implementation of these algorithms in Stan and R. We also extend the same method to propose the definition of an *indirect prior* and discuss the situations where it can be useful.},
  langid = {american},
  keywords = {Bayesian inference,Chebyshev polynomials,density quantile function,indirect likelihood,indirect prior,Other Statistics and Probability,Physical Sciences and Mathematics,Probability,proxy-rootfinding,quantile density function,quantile distribution,quantile functions,read,Statistical Methodology,Statistical Theory,Statistics and Probability,uncited-but-relevant}
}

@article{piironen_sparsity_2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  pages = {5018--5051},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/17-EJS1337SI},
  urldate = {2022-03-10},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  keywords = {62F15,Bayesian inference,horseshoe prior,read,shrinkage priors,Sparse estimation}
}

@article{preece_new_1978,
  title = {A New Family of Mathematical Models Describing the Human Growth Curve},
  author = {Preece, M.A. and Baines, M.J.},
  year = {1978},
  month = jan,
  journal = {Annals of Human Biology},
  volume = {5},
  number = {1},
  pages = {1--24},
  publisher = {{Taylor \& Francis}},
  issn = {0301-4460},
  doi = {10.1080/03014467800002601},
  urldate = {2022-03-03},
  abstract = {A new family of mathematical functions to fit longitudinal growth data is described. All members derive from the differential equation dh/dt=s(t). (h1-h) where h1 is adult size and s(t) is a function of time. The form of s(t) is given by one of many functions, all solutions of differential equations, thus generating a family of different models.Three versions were compared. All were superior to previously described models. Model 1, in which s(t) was defined by ds/dt=(s1-s)(s-s0) was especially accurate and robust, containing only five parameters to describe growth in stature from age two to maturity.Derived ``biological'' parameters such as Peak Height Velocity were very consistent between these three members of the family but, in some cases, differed significantly from previous estimates.},
  pmid = {646321},
  keywords = {read}
}

@article{presanis_conflict_2013,
  title = {Conflict {{Diagnostics}} in {{Directed Acyclic Graphs}}, with {{Applications}} in {{Bayesian Evidence Synthesis}}},
  author = {Presanis, Anne M. and Ohlssen, David and Spiegelhalter, David J. and De Angelis, Daniela},
  year = {2013},
  month = aug,
  journal = {Statistical Science},
  volume = {28},
  number = {3},
  pages = {376--397},
  issn = {0883-4237},
  doi = {10.1214/13-STS426},
  urldate = {2020-07-29},
  abstract = {Complex stochastic models represented by directed acyclic graphs (DAGs) are increasingly employed to synthesise multiple, imperfect and disparate sources of evidence, to estimate quantities that are difficult to measure directly. The various data sources are dependent on shared parameters and hence have the potential to conflict with each other, as well as with the model. In a Bayesian framework, the model consists of three components: the prior distribution, the assumed form of the likelihood and structural assumptions. Any of these components may be incompatible with the observed data. The detection and quantification of such conflict and of data sources that are inconsistent with each other is therefore a crucial component of the model criticism process. We first review Bayesian model criticism, with a focus on conflict detection, before describing a general diagnostic for detecting and quantifying conflict between the evidence in different partitions of a DAG. The diagnostic is a p-value based on splitting the information contributing to inference about a ``separator'' node or group of nodes into two independent groups and testing whether the two groups result in the same inference about the separator node(s). We illustrate the method with three comprehensive examples: an evidence synthesis to estimate HIV prevalence; an evidence synthesis to estimate influenza case-severity; and a hierarchical growth model for rat weights.},
  langid = {english},
  keywords = {read}
}

@article{price_global_1983,
  title = {Global Optimization by Controlled Random Search},
  author = {Price, W. L.},
  year = {1983},
  month = jul,
  journal = {Journal of Optimization Theory and Applications},
  volume = {40},
  number = {3},
  pages = {333--348},
  issn = {1573-2878},
  doi = {10.1007/BF00933504},
  urldate = {2022-08-09},
  abstract = {The paper describes a new version, known as CRS2, of the author's controlled random search procedure for global optimization (CRS). The new procedure is simpler and requires less computer storage than the original version, yet it has a comparable performance. The results of comparative trials of the two procedures, using a set of standard test problems, are given. These test problems are examples of unconstrained optimization. The controlled random search procedure can also be effective in the presence of constraints. The technique of constrained optimization using CRS is illustrated by means of examples taken from the field of electrical engineering.},
  langid = {english},
  keywords = {global search,nonlinear programming,Numerical optimization}
}

@article{probst_tunability_2019,
  title = {Tunability: {{Importance}} of {{Hyperparameters}} of {{Machine Learning Algorithms}}},
  author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  number = {20},
  pages = {32},
  abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.},
  langid = {english}
}

@article{quinlan_class_2021,
  title = {On a Class of Repulsive Mixture Models},
  author = {Quinlan, Jos{\'e} J. and Quintana, Fernando A. and Page, Garritt L.},
  year = {2021},
  month = jun,
  journal = {TEST. An Official Journal of the Spanish Society of Statistics and Operations Research},
  volume = {30},
  number = {2},
  pages = {445--461},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/s11749-020-00726-y},
  urldate = {2022-09-01},
  abstract = {Finite or infinite mixture models are routinely used in Bayesian statistical practice for tasks such as clustering or density estimation. Such models are very attractive due to their flexibility and tractability. However, a common problem in fitting these or other discrete models to data is that they tend to produce a large number of overlapping clusters. Some attention has been given in the statistical literature to models that include a repulsive feature, i.e., that encourage separation of mixture components. We study here a method that has been shown to achieve this goal without sacrificing flexibility or model fit. The model is a special case of Gibbs measures, with a parameter that controls the level of repulsion that allows construction of d-dimensional probability densities whose coordinates tend to repel each other. This approach was successfully used for density regression in Quinlan et al. (J Stat Comput Simul 88(15):2931\textendash 2947, 2018). We detail some of the global properties of the repulsive family of distributions and offer some further insight by means of a small simulation study.},
  langid = {english}
}

@misc{ramsay_fda_2022,
  title = {{{{\textsc{fda}}}}: Functional Data Analysis},
  shorttitle = {Fda},
  author = {Ramsay, J. O. and Graves, Spencer and Hooker, Giles},
  year = {2022},
  month = apr,
  urldate = {2022-05-31},
  abstract = {These functions were developed to support functional data analysis as described in Ramsay, J. O. and Silverman, B. W. (2005) Functional Data Analysis. New York: Springer and in Ramsay, J. O., Hooker, Giles, and Graves, Spencer (2009). Functional Data Analysis with R and Matlab (Springer). The package includes data sets and script files working many examples including all but one of the 76 figures in this latter book. Matlab versions are available by ftp from {$<$}https://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/{$>$}.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {FunctionalData}
}

@misc{rcoreteam_language_2023,
  title = {{{{\textsc{r}}}}: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2023},
  address = {{Vienna, Austria}},
  howpublished = {R Foundation for Statistical Computing}
}

@article{roos_sensitivity_2015,
  title = {Sensitivity Analysis for {{Bayesian}} Hierarchical Models},
  author = {Roos, Ma{\l}gorzata and Martins, Thiago G. and Held, Leonhard and Rue, H{\aa}vard},
  year = {2015},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {10},
  number = {2},
  pages = {321--349},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/14-BA909},
  urldate = {2022-06-14},
  abstract = {Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.},
  keywords = {Base prior,Bayesian hierarchical models,Bayesian robustness,Calibration,formal local sensitivity measure,Hellinger distance,Identifiability,overparametrisation,read}
}

@article{rousseeuw_alternatives_1993,
  title = {Alternatives to the Median Absolute Deviation},
  author = {Rousseeuw, Peter J. and Croux, Christophe},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {424},
  pages = {1273--1283},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291267},
  urldate = {2022-08-29},
  abstract = {In robust estimation one frequently needs an initial or auxiliary estimate of scale. For this one usually takes the median absolute deviation {$<$}tex-math{$>\$\backslash$}mathrm\{MAD\}\_n = 1.4826 \textbackslash operatorname\{med\}\_i\textbackslash\{| x\_i - \textbackslash operatorname\{med\}\_jx\_j|\textbackslash\}\${$<$}/tex-math{$>$}, because it has a simple explicit formula, needs little computation time, and is very robust as witnessed by its bounded influence function and its 50\% breakdown point. But there is still room for improvement in two areas: the fact that MAD\textsubscript{n} is aimed at symmetric distributions and its low (37\%) Gaussian efficiency. In this article we set out to construct explicit and 50\% breakdown scale estimators that are more efficient. We consider the estimator {$<$}tex-math{$>\$$}S\_n = 1.1926 \textbackslash operatorname\{med\}\_i\textbackslash\{\textbackslash operatorname\{med\}\_j|x\_i - x\_j|\textbackslash\}\${$<$}/tex-math{$>$} and the estimator Q\textsubscript{n} given by the .25 quantile of the distances {$<$}latex{$>\$\backslash\lbrace$}|x\_i - x\_j|; i {$<$} j\textbackslash\vphantom\{\}\${$<$}/latex{$>$}. Note that S\textsubscript{n} and Q\textsubscript{n} do not need any location estimate. Both S\textsubscript{n} and Q\textsubscript{n} can be computed using O(n log n) time and O(n) storage. The Gaussian efficiency of S\textsubscript{n} is 58\%, whereas Q\textsubscript{n} attains 82\%. We study S\textsubscript{n} and Q\textsubscript{n} by means of their influence functions, their bias curves (for implosion as well as explosion), and their finite-sample performance. Their behavior is also compared at non-Gaussian models, including the negative exponential model where S\textsubscript{n} has a lower gross-error sensitivity than the MAD.},
  jstor = {2291267}
}

@misc{rowe_futilelogger_2016,
  title = {{{{\textsc{futile.logger}}}}: A {{Logging Utility}} for {{{\textsc{R}}}}},
  shorttitle = {Futile.{{Logger}}},
  author = {Rowe, Brian Lee Yung},
  year = {2016},
  month = jul,
  urldate = {2022-07-07},
  abstract = {Provides a simple yet powerful logging utility. Based loosely on log4j, futile.logger takes advantage of R idioms to make logging a convenient and easy to use replacement for cat and print statements.},
  copyright = {LGPL-3}
}

@article{sailynoja_graphical_2022,
  title = {Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison},
  author = {S{\"a}ilynoja, Teemu and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2022},
  month = mar,
  journal = {Statistics and Computing},
  volume = {32},
  number = {2},
  pages = {32},
  issn = {1573-1375},
  doi = {10.1007/s11222-022-10090-6},
  urldate = {2022-06-27},
  abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation- and optimization-based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample, which is useful, for example, for simulation-based calibration. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is useful, for example, as a complementary diagnostic in multi-chain Markov chain Monte Carlo (MCMC) convergence diagnostics, where most currently used convergence diagnostics provide a single diagnostic value, but do not usually offer insight into the nature of the deviation. We provide numerical experiments to assess the properties of the tests using both simulated and real-world data and give recommendations on their practical application in computational statistics workflows.},
  langid = {english},
  keywords = {ECDF,MCMC convergence diagnostic,PIT,read,Simulation-based calibration,Uniformity test}
}

@article{salinas_quantile-based_2021,
  title = {A {{Quantile-based Approach}} for {{Hyperparameter Transfer Learning}}},
  author = {Salinas, David and Shen, Huibin and Perrone, Valerio},
  year = {2021},
  month = apr,
  journal = {arXiv:1909.13595 [cs, stat]},
  eprint = {1909.13595},
  primaryclass = {cs, stat},
  urldate = {2022-04-25},
  abstract = {Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance objectives of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different \textbackslash emph\{datasets\} as well as different \textbackslash emph\{objectives\}. The main idea is to regress the mapping from hyperparameter to objective quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this mapping: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple objectives such as latency and accuracy, steering the hyperparameters optimization toward faster predictions for the same level of accuracy. Extensive experiments demonstrate significant improvements over state-of-the-art methods for both hyperparameter optimization and neural architecture search.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{scholz_ksamples_2019,
  title = {{{{\textsc{kSamples}}}}: {{K-Sample Rank Tests}} and Their {{Combinations}}},
  shorttitle = {{{kSamples}}},
  author = {Scholz, Fritz and Zhu, Angie},
  year = {2019},
  month = may,
  urldate = {2022-02-08},
  abstract = {Compares k samples using the Anderson-Darling test, Kruskal-Wallis type tests with different rank score criteria, Steel's multiple comparison test, and the Jonckheere-Terpstra (JT) test. It computes asymptotic, simulated or (limited) exact P-values, all valid under randomization, with or without ties, or conditionally under random sampling from populations, given the observed tie pattern. Except for Steel's test and the JT test it also combines these tests across several blocks of samples. Also analyzed are 2 x t contingency tables and their blocked combinations using the Kruskal-Wallis criterion. Steel's test is inverted to provide simultaneous confidence bounds for shift parameters. A plotting function compares tail probabilities obtained under asymptotic approximation with those obtained via simulation or exact calculations.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  urldate = {2022-08-04},
  abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  langid = {english}
}

@article{simpson_penalising_2017,
  title = {Penalising Model Component Complexity: {{A}} Principled, Practical Approach to Constructing Priors},
  shorttitle = {Penalising Model Component Complexity},
  author = {Simpson, Daniel and Rue, H{\aa}vard and Riebler, Andrea and Martins, Thiago G. and S{\o}rbye, Sigrunn H.},
  year = {2017},
  month = feb,
  journal = {Statistical Science},
  volume = {32},
  number = {1},
  pages = {1--28},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/16-STS576},
  urldate = {2021-07-20},
  abstract = {In this paper, we introduce a new concept for constructing prior distributions. We exploit the natural nested structure inherent to many model components, which defines the model component to be a flexible extension of a base model. Proper priors are defined to penalise the complexity induced by deviating from the simpler base model and are formulated after the input of a user-defined scaling parameter for that model component, both in the univariate and the multivariate case. These priors are invariant to reparameterisations, have a natural connection to Jeffreys' priors, are designed to support Occam's razor and seem to have excellent robustness properties, all which are highly desirable and allow us to use this approach to define default prior distributions. Through examples and theoretical results, we demonstrate the appropriateness of this approach and how it can be applied in various situations.},
  keywords = {Bayesian theory,disease mapping,hierarchical models,information geometry,interpretable prior distributions,prior on correlation matrices,read}
}

@inproceedings{snoek_input_2014,
  title = {Input Warping for {{Bayesian}} Optimization of Non-Stationary Functions},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Snoek, Jasper and Swersky, Kevin and Zemel, Rich and Adams, Ryan},
  year = {2014},
  month = jun,
  pages = {1674--1682},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2022-04-25},
  abstract = {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space", to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.},
  langid = {english}
}

@misc{souris_soft_2019,
  title = {The {{Soft Multivariate Truncated Normal Distribution}} with {{Applications}} to {{Bayesian Constrained Estimation}}},
  author = {Souris, Allyson and Bhattacharya, Anirban and Pati, Debdeep},
  year = {2019},
  month = sep,
  number = {arXiv:1807.09155},
  eprint = {1807.09155},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2022-06-22},
  abstract = {We propose a new distribution, called the soft tMVN distribution, which provides a smooth approximation to the truncated multivariate normal (tMVN) distribution with linear constraints. An efficient blocked Gibbs sampler is developed to sample from the soft tMVN distribution in high dimensions. We provide theoretical support to the approximation capability of the soft tMVN and provide further empirical evidence thereof. The soft tMVN distribution can be used to approximate simulations from a multivariate truncated normal distribution with linear constraints, or itself as a prior in shape-constrained problems.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation}
}

@misc{stan_development_team_rstan_2021,
  title = {{{{\textsc{rstan}}}}: The {{R}} Interface to {{Stan}}},
  author = {{Stan Development Team}},
  year = {2021},
  keywords = {read}
}

@misc{stan_development_team_stan_2022,
  title = {Stan Modeling Language: {{User}}'s Guide and Reference Manual},
  author = {{Stan Development Team}},
  year = {2022},
  keywords = {read}
}

@article{stefan_practical_2022,
  title = {Practical Challenges and Methodological Flexibility in Prior Elicitation},
  author = {Stefan, Angelika M. and Evans, Nathan J. and Wagenmakers, Eric-Jan},
  year = {2022},
  journal = {Psychological Methods},
  volume = {27},
  number = {2},
  pages = {177--197},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000354},
  abstract = {The Bayesian statistical framework requires the specification of prior distributions, which reflect predata knowledge about the relative plausibility of different parameter values. As prior distributions influence the results of Bayesian analyses, it is important to specify them with care. Prior elicitation has frequently been proposed as a principled method for deriving prior distributions based on expert knowledge. Although prior elicitation provides a theoretically satisfactory method of specifying prior distributions, there are several implicit decisions that researchers need to make at different stages of the elicitation process, each of them constituting important researcher degrees of freedom. Here, we discuss some of these decisions and group them into 3 categories: decisions about (a) the setup of the prior elicitation; (b) the core elicitation process; and (c) combination of elicited prior distributions from different experts. Importantly, different decision paths could result in greatly varying priors elicited from the same experts. Hence, researchers who wish to perform prior elicitation are advised to carefully consider each of the practical decisions before, during, and after the elicitation process. By explicitly outlining the consequences of these practical decisions, we hope to raise awareness for methodological flexibility in prior elicitation and provide researchers with a more structured approach to navigate the decision paths in prior elicitation. Making the decisions explicit also provides the foundation for further research that can identify evidence-based best practices that may eventually reduce the methodologically flexibility in prior elicitation. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Awareness,Bayesian Analysis,Best Practices,Freedom,Methodology,read,Statistical Probability}
}

@article{stein_large_1987,
  title = {Large Sample Properties of Simulations Using {{Latin}} Hypercube Sampling},
  author = {Stein, Michael},
  year = {1987},
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  volume = {29},
  number = {2},
  pages = {143--151},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269769},
  urldate = {2022-08-09},
  abstract = {Latin hypercube sampling (McKay, Conover, and Beckman 1979) is a method of sampling that can be used to produce input values for estimation of expectations of functions of output variables. The asymptotic variance of such an estimate is obtained. The estimate is also shown to be asymptotically normal. Asymptotically, the variance is less than that obtained using simple random sampling, with the degree of variance reduction depending on the degree of additivity in the function being integrated. A method for producing Latin hypercube samples when the components of the input variables are statistically dependent is also described. These techniques are applied to a simulation of the performance of a printer actuator.},
  jstor = {1269769}
}

@article{stigler_thomas_1982,
  title = {Thomas {{Bayes}}'s {{Bayesian Inference}}},
  author = {Stigler, Stephen M.},
  year = {1982},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {145},
  number = {2},
  pages = {250--258},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9238},
  doi = {10.2307/2981538},
  urldate = {2022-01-16},
  abstract = {We consider Thomas Bayes's famous Scholium\textendash his argument in defence of an a priori uniform distribution for an unknown probability, and argue that critics (R. A. Fisher) and friends (Karl Pearson, Harold Jeffreys) alike have misinterpreted the argument as an appeal to the principle of insufficient reason, and that Bayes's actual argument is free from the principal defect it has been charged with. True "Bayesian Inference" is found to differ considerably from and perhaps be logically preferable to modern perceptions of it.},
  jstor = {2981538},
  keywords = {read}
}

@article{sun_functional_2019,
  title = {Functional {{Variational Bayesian Neural Networks}}},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.05779 [cs, stat]},
  eprint = {1903.05779},
  primaryclass = {cs, stat},
  urldate = {2022-02-16},
  abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{sweeting_predictive_2008,
  title = {On Predictive Probability Matching Priors},
  author = {Sweeting, Trevor J.},
  year = {2008},
  month = jan,
  journal = {Pushing the Limits of Contemporary Statistics: Contributions in Honor of Jayanta K. Ghosh},
  volume = {3},
  pages = {46--60},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/074921708000000048},
  urldate = {2022-07-14},
  abstract = {{$<$}!\textendash{} *** Custom HTML *** \textendash{$><$}p{$>$} We revisit the question of priors that achieve approximate matching of Bayesian and frequentist predictive probabilities. Such priors may be thought of as providing frequentist calibration of Bayesian prediction or simply as devices for producing frequentist prediction regions. Here we analyse the \emph{O}(\emph{n}{$^{-1}$}) term in the expansion of the coverage probability of a Bayesian prediction region, as derived in [\emph{Ann. Statist.} \textbf{28} (2000) 1414\textendash 1426]. Unlike the situation for parametric matching, asymptotic predictive matching priors may depend on the level \emph{{$\alpha$}}. We investigate \emph{uniformly predictive matching priors} (UPMPs); that is, priors for which this \emph{O}(\emph{n}{$^{-1}$}) term is zero for all \emph{{$\alpha$}}. It was shown in [\emph{Ann. Statist.} \textbf{28} (2000) 1414\textendash 1426] that, in the case of quantile matching and a scalar parameter, if such a prior exists then it must be Jeffreys' prior. In the present article we investigate UPMPs in the multiparameter case and present some general results about the form, and uniqueness or otherwise, of UPMPs for both quantile and highest predictive density matching. {$<$}/p{$>$}}
}

@article{terenin_noninformative_2017,
  title = {A {{Noninformative Prior}} on a {{Space}} of {{Distribution Functions}}},
  author = {Terenin, Alexander and Draper, David},
  year = {2017},
  month = jul,
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {19},
  number = {8},
  eprint = {1703.04661},
  primaryclass = {math, stat},
  pages = {391},
  issn = {1099-4300},
  doi = {10.3390/e19080391},
  urldate = {2022-08-01},
  abstract = {In a given problem, the Bayesian statistical paradigm requires the specification of a prior distribution that quantifies relevant information about the unknowns of main interest external to the data. In cases where little such information is available, the problem under study may possess an invariance under a transformation group that encodes a lack of information, leading to a unique prior\textemdash this idea was explored at length by E.T. Jaynes. Previous successful examples have included location-scale invariance under linear transformation, multiplicative invariance of the rate at which events in a counting process are observed, and the derivation of the Haldane prior for a Bernoulli success probability. In this paper we show that this method can be extended, by generalizing Jaynes, in two ways: (1) to yield families of approximately invariant priors, and (2) to the infinite-dimensional setting, yielding families of priors on spaces of distribution functions. Our results can be used to describe conditions under which a particular Dirichlet Process posterior arises from an optimal Bayesian analysis, in the sense that invariances in the prior and likelihood lead to one and only one posterior distribution.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory}
}

@article{thomas_probabilistic_2020,
  title = {Probabilistic Elicitation of Expert Knowledge through Assessment of Computer Simulations},
  author = {Thomas, Owen and Pesonen, Henri and Corander, Jukka},
  year = {2020},
  month = mar,
  journal = {arXiv:2002.10902 [stat]},
  eprint = {2002.10902},
  primaryclass = {stat},
  urldate = {2022-01-31},
  abstract = {We present a new method for probabilistic elicitation of expert knowledge using binary responses of human experts assessing simulated data from a statistical model, where the parameters are subject to uncertainty. The binary responses describe either the absolute realism of individual simulations or the relative realism of a pair of simulations in the two alternative versions of out approach. Each version provides a nonparametric representation of the expert belief distribution over the values of a model parameter, without demanding the assertion of any opinion on the parameter values themselves. Our framework also integrates the use of active learning to efficiently query the experts, with the possibility to additionally provide a useful misspecification diagnostic. We validate both methods on an automatic expert judging a binomial distribution, and on human experts judging the distribution of voters across political parties in the United States and Norway. Both methods provide flexible and meaningful representations of the human experts' beliefs, correctly identifying the higher dispersion of voters between parties in Norway.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Applications,Statistics - Computation,Statistics - Methodology}
}

@article{tuddenham_physical_1954,
  title = {Physical Growth of {{California}} Boys and Girls from Birth to Eighteen Years},
  author = {Tuddenham, R. D. and Snyder, M. M.},
  year = {1954},
  journal = {Publications in Child Development. University of California, Berkeley},
  volume = {1},
  number = {2},
  pages = {183--364},
  langid = {english},
  pmid = {13217130},
  keywords = {California,Child,Female,Growth,GROWTH/in infant and child,Humans,Infant,Male,Physiological Phenomena,read}
}

@article{tversky_judgment_1974,
  title = {Judgment under {{Uncertainty}}: {{Heuristics}} and {{Biases}}: {{Biases}} in Judgments Reveal Some Heuristics of Thinking under Uncertainty.},
  shorttitle = {Judgment under {{Uncertainty}}},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1974},
  month = sep,
  journal = {Science (New York, N.Y.)},
  volume = {185},
  number = {4157},
  pages = {1124--1131},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.185.4157.1124},
  urldate = {2022-06-15},
  langid = {english},
  keywords = {read}
}

@article{van_zundert_prior_2022,
  title = {Prior Predictive Checks for the Method of Covariances in {{Bayesian}} Mediation Analysis},
  author = {{van Zundert}, Camiel and Somer, Emma and Mio{\v c}evi{\'c}, Milica},
  year = {2022},
  month = may,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {3},
  pages = {428--437},
  publisher = {{Routledge}},
  issn = {1070-5511},
  doi = {10.1080/10705511.2021.1977648},
  urldate = {2022-06-13},
  abstract = {Bayesian mediation analysis using the method of covariances requires specifying a prior for the covariance matrix of the independent variable, mediator, and outcome. Using a conjugate inverse-Wishart prior has been the norm, even though this choice assumes equal levels of informativeness for all elements in the covariance matrix. This paper describes separation strategy priors for the single mediator model, develops a Prior Predictive Check (PrPC) for inverse-Wishart and separation strategy priors, and implements the PrPC in a Shiny app. An empirical example illustrates the possibilities in the app. Guidelines are provided for selecting the optimal prior specification for the prior knowledge researchers wish to encode.},
  keywords = {Bayesian Mediation Analysis,Method of Covariances,Prior Predictive Check,read,Separation Strategy,Shiny app}
}

@article{vehtari_pareto_2021,
  ids = {vehtari_pareto_2019},
  title = {Pareto Smoothed Importance Sampling},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  year = {2021},
  month = feb,
  journal = {arXiv:1507.02646 [stat]},
  eprint = {1507.02646},
  primaryclass = {stat},
  urldate = {2021-09-20},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arxiv},
  keywords = {read,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@article{von_mises_asymptotic_1947,
  title = {On the Asymptotic Distribution of Differentiable Statistical Functions},
  author = {{von Mises}, Richard},
  year = {1947},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {3},
  pages = {309--348},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177730385},
  urldate = {2022-06-27},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {read}
}

@article{wan_adversarial_2021,
  title = {Adversarial {{Attacks}} on {{Graph Classification}} via {{Bayesian Optimisation}}},
  author = {Wan, Xingchen and Kenlay, Henry and Ru, Binxin and Blaas, Arno and Osborne, Michael A. and Dong, Xiaowen},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.02842 [cs, stat]},
  eprint = {2111.02842},
  primaryclass = {cs, stat},
  urldate = {2021-11-14},
  abstract = {Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{wang_using_2018,
  title = {Using History Matching for Prior Choice},
  author = {Wang, Xueou and Nott, David J. and Drovandi, Christopher C. and Mengersen, Kerrie and Evans, Michael},
  year = {2018},
  month = oct,
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  volume = {60},
  number = {4},
  pages = {445--460},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2017.1421587},
  urldate = {2021-07-20},
  abstract = {It can be important in Bayesian analyses of complex models to construct informative prior distributions which reflect knowledge external to the data at hand. Nevertheless, how much prior information an analyst can elicit from an expert will be limited due to constraints of time, cost and other factors. This article develops effective numerical methods for exploring reasonable choices of a prior distribution from a parametric class, when prior information is specified in the form of some limited constraints on prior predictive distributions, and where these prior predictive distributions are analytically intractable. The methods developed may be thought of as a novel application of the ideas of history matching, a technique developed in the literature on assessment of computer models. We illustrate the approach in the context of logistic regression and sparse signal shrinkage prior distributions for high-dimensional linear models.},
  langid = {english},
  keywords = {read}
}

@article{wickham_welcome_2019,
  title = {Welcome to the {{Tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  issn = {2475-9066},
  doi = {10.21105/joss.01686},
  urldate = {2022-07-07},
  abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next.},
  langid = {english}
}

@article{winkler_assessment_1967,
  title = {The Assessment of Prior Distributions in {{Bayesian}} Analysis},
  author = {Winkler, Robert L.},
  year = {1967},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {62},
  number = {319},
  pages = {776--800},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1967.10500894},
  urldate = {2022-06-15},
  abstract = {In the Bayesian framework, quantified judgments about uncertainty are an indispensable input to methods of statistical inference and decision. Ultimately, all components of the formal mathematical models underlying inferential procedures represent quantified judgments. In this study, the focus is on just one component, the prior distribution, and on some of the problems of assessment that arise when a person tries to express prior distributions in quantitative form. The objective is to point toward assessment procedures that can actually be used. One particular type of statistical problem is considered and several techniques of assessment are presented, together with the necessary instruction so that these techniques can be understood and applied. A questionnaire is developed and used in a study in which people actually assess prior distributions. The results indicate that, by and large, it is feasible to question people about subjective prior probability distributions, although this depends on the assessor and on the assessment technique(s) used. A revised questionnaire, which is aimed at potential users of the assessment procedures and future investigators in the area of probability assessment, is presented.},
  keywords = {read}
}

@article{wong_frequentist_2017,
  title = {A Frequentist Approach to Computer Model Calibration},
  author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
  year = {2017},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {79},
  number = {2},
  pages = {635--648},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  urldate = {2022-06-13},
  abstract = {The paper considers the computer model calibration problem and provides a general frequentist solution. Under the framework proposed, the data model is semiparametric with a non-parametric discrepancy function which accounts for any discrepancy between physical reality and the computer model. In an attempt to solve a fundamentally important (but often ignored) identifiability issue between the computer model parameters and the discrepancy function, the paper proposes a new and identifiable parameterization of the calibration problem. It also develops a two-step procedure for estimating all the relevant quantities under the new parameterization. This estimation procedure is shown to enjoy excellent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantification, bootstrapping is adopted to construct confidence regions for the quantities of interest. The practical performance of the methodology is illustrated through simulation examples and an application to a computational fluid dynamics model.},
  jstor = {44682529},
  keywords = {broken-with-latex,uncited-but-relevant}
}

@article{wood_thin_2003,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  year = {2003},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00374},
  urldate = {2022-08-31},
  abstract = {Summary. I discuss the production of low rank smoothers for d {$\geq$} 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with `knot placement' that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics.},
  langid = {english},
  keywords = {Generalized additive model,Regression spline,Thin plate spline}
}

@inproceedings{wu_parallel_2016,
  title = {The {{Parallel Knowledge Gradient Method}} for {{Batch Bayesian Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Jian and Frazier, Peter},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-09-07},
  abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm \textemdash{} the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.}
}

@misc{ypma_nloptr_2022,
  title = {{{{\textsc{nloptr}}}}: R Interface to {{NLopt}}},
  shorttitle = {Nloptr},
  author = {Ypma, Jelmer and Johnson, Steven G. and Borchers, Hans W. and Eddelbuettel, Dirk and Ripley, Brian and Hornik, Kurt and Chiquet, Julien and Adler, Avraham and Dai, Xiongtao and Stamm, Aymeric and Ooms, Jeroen},
  year = {2022},
  month = jan,
  urldate = {2022-04-19},
  abstract = {Solve optimization problems using an R interface to NLopt. NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. See {$<$}https://nlopt.readthedocs.io/en/latest/NLopt\_Algorithms/{$>$} for more information on the available algorithms. Building from included sources requires 'CMake'. On Linux and 'macOS', if a suitable system build of NLopt (2.7.0 or later) is found, it is used; otherwise, it is built from included sources via 'CMake'. On Windows, NLopt is obtained through 'rwinlib' for 'R {$<$}= 4.1.x' or grabbed from the 'Rtools42 toolchain' for 'R {$>$}= 4.2.0'.},
  copyright = {LGPL ({$\geq$} 3)},
  keywords = {Optimization}
}

@misc{zaefferer_mspot_2012,
  title = {{{{\textsc{mspot}}}}: Multi-Criteria Sequential Parameter Optimization},
  author = {Zaefferer, Martin and {Bartz-Beielstein}, Thomas and Friese, Martina and Naujoks, Boris and Flasch, Oliver},
  year = {2012},
  abstract = {Many relevant industrial optimization tasks feature more than just one quality criterion. State-of-the-art multi-criteria optimization algorithms require a relatively large number of function evaluations (usually more than 105) to approximate Pareto fronts. Due to high cost or time consumption this large amount of function evaluations is not always available. Therefore, it is obvious to combine techniques such as Sequential Parameter Optimization (SPO), which need a very small number of function evaluations only, with techniques from evolutionary multi-criteria optimization (EMO). In this paper, we show how EMO techniques can be e ciently integrated into the framework of the SPO Toolbox (SPOT). We discuss advantages of this approach in comparison to state-of-the-art optimizers. Moreover, with the resulting capability to allow competing objectives, the opportunity arises to not only aim for the best, but also for the most robust solution. Herein we present an approach to optimize not only the quality of the solution, but also its robustness, taking these two goals as objectives for multi-criteria optimization into account.},
  langid = {english},
  keywords = {read}
}

@article{zemel_application_1994,
  title = {Application of the {{Preece-Baines}} Growth Model to Cross-Sectional Data: {{Problems}} of Validity and Interpretation},
  shorttitle = {Application of the {{Preece-Baines}} Growth Model to Cross-Sectional Data},
  author = {Zemel, Babette S. and Johnston, Francis E.},
  year = {1994},
  journal = {American Journal of Human Biology},
  volume = {6},
  number = {5},
  pages = {563--570},
  issn = {1520-6300},
  doi = {10.1002/ajhb.1310060504},
  urldate = {2022-03-02},
  abstract = {The Preece-Baines growth model (PBGM) is a family of curves that conform to the shape of the human growth curve. It is most often used to analyze longitudinal records on individuals, but it can be applied to cross-sectional data from population surveys. The purpose of this study was to determine empirically the validity of applying the PBGM to cross-sectional data to make inferences about the timing and nature of the adolescent growth spurt. Longitudinal records (n = 339) from the Third Harvard Growth Study were analyzed individually using the PBGM. Results for each sex were pooled to characterize longitudinal growth in this population. Ten independent random cross-sectional samples were generated from the same data set. These were divided by sex and the age means for height were analyzed using the PBGM. Comparisons between the pooled longitudinal estimates and those from the random cross-sectional samples indicated that the PBGM can accurately estimate the age at peak height velocity in males, as well as other characteristics of the adolescent growth spurt when using cross-sectional data. In females, spurious results were obtained when cross-sectional data are used. Possible sources of bias are discussed. \textcopyright{} 1994 Wiley-Liss, Inc.},
  langid = {english},
  keywords = {broken-with-latex}
}

@article{zhang_bayesian_2022,
  title = {Bayesian Regression Using a Prior on the Model Fit: {{The R2-D2}} Shrinkage Prior},
  shorttitle = {Bayesian Regression Using a Prior on the Model Fit},
  author = {Zhang, Yan Dora and Naughton, Brian P. and Bondell, Howard D. and Reich, Brian J.},
  year = {2022},
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {538},
  pages = {862--874},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1825449},
  urldate = {2022-01-17},
  abstract = {Prior distributions for high-dimensional linear regression require specifying a joint distribution for the unobserved regression coefficients, which is inherently difficult. We instead propose a new class of shrinkage priors for linear regression via specifying a prior first on the model fit, in particular, the coefficient of determination, and then distributing through to the coefficients in a novel way. The proposed method compares favorably to previous approaches in terms of both concentration around the origin and tail behavior, which leads to improved performance both in posterior contraction and in empirical performance. The limiting behavior of the proposed prior is 1/x, both around the origin and in the tails. This behavior is optimal in the sense that it simultaneously lies on the boundary of being an improper prior both in the tails and around the origin. None of the existing shrinkage priors obtain this behavior in both regions simultaneously. We also demonstrate that our proposed prior leads to the same near-minimax posterior contraction rate as the spike-and-slab prior. Supplementary materials for this article are available online.},
  keywords = {Beta-prime distribution,Coefficient of determination,Global-local shrinkage,High-dimensional regression,read}
}

@article{zhang_variable_2018,
  title = {Variable Selection via Penalized Credible Regions with {{Dirichlet}}\textendash{{Laplace}} Global-Local Shrinkage Priors},
  author = {Zhang, Yan and Bondell, Howard D.},
  year = {2018},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {3},
  issn = {1936-0975},
  doi = {10.1214/17-BA1076},
  urldate = {2022-02-02},
  abstract = {The method of Bayesian variable selection via penalized credible regions separates model fitting and variable selection. The idea is to search for the sparsest solution within the joint posterior credible regions. Although the approach was successful, it depended on the use of conjugate normal priors. More recently, improvements in the use of global-local shrinkage priors have been made for highdimensional Bayesian variable selection. In this paper, we incorporate global-local priors into the credible region selection framework. The Dirichlet\textendash Laplace (DL) prior is adapted to linear regression. Posterior consistency for the normal and DL priors are shown, along with variable selection consistency. We further introduce a new method to tune hyperparameters in prior distributions for linear regression. We propose to choose the hyperparameters to minimize a discrepancy between the induced distribution on R-square and a prespecified target distribution. Prior elicitation on R-square is more natural, particularly when there are a large number of predictor variables in which elicitation on that scale is not feasible. For a normal prior, these hyperparameters are available in closed form to minimize the Kullback\textendash Leibler divergence between the distributions.},
  langid = {english},
  keywords = {read}
}
